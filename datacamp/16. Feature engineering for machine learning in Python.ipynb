{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Why generate features?**\n",
    "___\n",
    "- Different types of data\n",
    "    - continuous: integers or floats\n",
    "    - categorical: one of a limited set of values\n",
    "    - ordinal: ranked values\n",
    "    - boolean: true/false values\n",
    "    - datetime: dates and times\n",
    "- course structure\n",
    "    - chapter 1: feature creation and extraction\n",
    "    - chapter 2: engineering messy data\n",
    "    - chapter 3: feature normalization\n",
    "    - chapter 4: working with text features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Getting to know your data\n",
    "\n",
    "#Pandas is one the most popular packages used to work with tabular\n",
    "#data in Python. It is generally imported using the alias pd and can\n",
    "#be used to load a CSV (or other delimited files) using read_csv().\n",
    "\n",
    "#You will be working with a modified subset of the Stackoverflow\n",
    "#survey response data in the first three chapters of this course.\n",
    "#This data set records the details, and preferences of thousands of\n",
    "#users of the StackOverflow website.\n",
    "\n",
    "# Import pandas\n",
    "#import pandas as pd\n",
    "\n",
    "# Import so_survey_csv into so_survey_df\n",
    "#so_survey_df = pd.read_csv(so_survey_csv)\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "#print(so_survey_df.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#          SurveyDate                                    FormalEducation  ConvertedSalary Hobby       Country  ...     VersionControl Age  Years Experience  Gender   RawSalary\n",
    "#    0  2/28/18 20:20           Bachelor's degree (BA. BS. B.Eng.. etc.)              NaN   Yes  South Africa  ...                Git  21                13    Male         NaN\n",
    "#    1  6/28/18 13:26           Bachelor's degree (BA. BS. B.Eng.. etc.)          70841.0   Yes       Sweeden  ...     Git;Subversion  38                 9    Male   70,841.00\n",
    "#    2    6/6/18 3:37           Bachelor's degree (BA. BS. B.Eng.. etc.)              NaN    No       Sweeden  ...                Git  45                11     NaN         NaN\n",
    "#    3    5/9/18 1:06  Some college/university study without earning ...          21426.0   Yes       Sweeden  ...  Zip file back-ups  46                12    Male   21,426.00\n",
    "#    4  4/12/18 22:41           Bachelor's degree (BA. BS. B.Eng.. etc.)          41671.0   Yes            UK  ...                Git  39                 7    Male  £41,671.00\n",
    "#\n",
    "#    [5 rows x 11 columns]\n",
    "#################################################\n",
    "\n",
    "# Print the data type of each column\n",
    "#print(so_survey_df.dtypes)\n",
    "\n",
    "#################################################\n",
    "#    SurveyDate                     object\n",
    "#    FormalEducation                object\n",
    "#    ConvertedSalary               float64\n",
    "#    Hobby                          object\n",
    "#    Country                        object\n",
    "#    StackOverflowJobsRecommend    float64\n",
    "#    VersionControl                 object\n",
    "#    Age                             int64\n",
    "#    Years Experience                int64\n",
    "#    Gender                         object\n",
    "#    RawSalary                      object\n",
    "#    dtype: object\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Selecting specific data types\n",
    "#Often a data set will contain columns with several different data\n",
    "#types (like the one you are working with). The majority of machine\n",
    "#learning models require you to have a consistent data type across\n",
    "#features. Similarly, most feature engineering techniques are\n",
    "#applicable to only one type of data at a time. For these reasons\n",
    "#among others, you will often want to be able to access just the\n",
    "#columns of certain types when working with a DataFrame.\n",
    "\n",
    "#The DataFrame (so_survey_df) from the previous exercise is available\n",
    "#in your workspace.\n",
    "\n",
    "# Create subset of only the numeric columns\n",
    "#so_numeric_df = so_survey_df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Print the column names contained in so_survey_df_num\n",
    "#print(so_numeric_df.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['ConvertedSalary', 'StackOverflowJobsRecommend', 'Age', 'Years Experience'], dtype='object')\n",
    "#################################################\n",
    "# In the next lesson, you will learn the most common ways of dealing\n",
    "#with categorical data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dealing with categorical features**\n",
    "___\n",
    "- encoding categorical features\n",
    "    - one-hot encoding\n",
    "        - converts n categories into n features\n",
    "        - explainable features\n",
    "        - problem of collinearity\n",
    "    - dummy encoding\n",
    "        - converts n categories into n-1 features\n",
    "        -\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#One-hot encoding and dummy variables\n",
    "\n",
    "#To use categorical variables in a machine learning model, you first\n",
    "#need to represent them in a quantitative way. The two most common\n",
    "#approaches are to one-hot encode the variables using or to use dummy\n",
    "#variables. In this exercise, you will create both types of encoding,\n",
    "#and compare the created column sets. We will continue using the same\n",
    "#DataFrame from previous lesson loaded as so_survey_df and focusing on\n",
    "#its Country column.\n",
    "\n",
    "# Convert the Country column to a one hot encoded Data Frame\n",
    "#one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')\n",
    "\n",
    "# Print the columns names\n",
    "#print(one_hot_encoded.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby', 'StackOverflowJobsRecommend', 'VersionControl', 'Age', 'Years Experience', 'Gender', 'RawSalary', 'OH_France', 'OH_India',\n",
    "#           'OH_Ireland', 'OH_Russia', 'OH_South Africa', 'OH_Spain', 'OH_Sweeden', 'OH_UK', 'OH_USA', 'OH_Ukraine'],\n",
    "#          dtype='object')\n",
    "#################################################\n",
    "\n",
    "# Create dummy variables for the Country column\n",
    "#dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')\n",
    "\n",
    "# Print the columns names\n",
    "#print(dummy.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby', 'StackOverflowJobsRecommend', 'VersionControl', 'Age', 'Years Experience', 'Gender', 'RawSalary', 'DM_India', 'DM_Ireland',\n",
    "#           'DM_Russia', 'DM_South Africa', 'DM_Spain', 'DM_Sweeden', 'DM_UK', 'DM_USA', 'DM_Ukraine'],\n",
    "#          dtype='object')\n",
    "#################################################\n",
    "#Did you notice that the column for France was missing when you\n",
    "#created dummy variables? Now you can choose to use one-hot encoding\n",
    "#or dummy variables where appropriate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dealing with uncommon categories\n",
    "\n",
    "#Some features can have many different categories but a very uneven\n",
    "#distribution of their occurrences. Take for example Data Science's\n",
    "#favorite languages to code in, some common choices are Python, R,\n",
    "#and Julia, but there can be individuals with bespoke choices, like\n",
    "#FORTRAN, C etc. In these cases, you may not want to create a feature\n",
    "#for each value, but only the more common occurrences.\n",
    "\n",
    "# Create a series out of the Country column\n",
    "#countries = so_survey_df['Country']\n",
    "\n",
    "# Get the counts of each category\n",
    "#country_counts = countries.value_counts()\n",
    "\n",
    "# Print the count values for each category\n",
    "#print(country_counts)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    South Africa    166\n",
    "#    USA             164\n",
    "#    Spain           134\n",
    "#    Sweeden         119\n",
    "#    France          115\n",
    "#    Russia           97\n",
    "#    India            95\n",
    "#    UK               95\n",
    "#    Ukraine           9\n",
    "#    Ireland           5\n",
    "#    Name: Country, dtype: int64\n",
    "#################################################\n",
    "\n",
    "# Create a mask for only categories that occur less than 10 times\n",
    "#mask = countries.isin(country_counts[country_counts < 10].index)\n",
    "\n",
    "# Print the top 5 rows in the mask series\n",
    "#print(mask.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    False\n",
    "#    1    False\n",
    "#    2    False\n",
    "#    3    False\n",
    "#    4    False\n",
    "#    Name: Country, dtype: bool\n",
    "#################################################\n",
    "\n",
    "# Label all other categories as Other\n",
    "#countries[mask] = 'Other'\n",
    "\n",
    "# Print the updated category counts\n",
    "#print(pd.value_counts(countries))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    South Africa    166\n",
    "#    USA             164\n",
    "#    Spain           134\n",
    "#    Sweeden         119\n",
    "#    France          115\n",
    "#    Russia           97\n",
    "#    India            95\n",
    "#    UK               95\n",
    "#    Other            14\n",
    "#    Name: Country, dtype: int64\n",
    "#################################################\n",
    "#now you can work with large data sets while grouping low frequency\n",
    "#categories."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Numeric variables**\n",
    "___\n",
    "- Types of numeric features\n",
    "    - age\n",
    "    - price\n",
    "    - counts\n",
    "    - geospatial data\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Binarizing columns\n",
    "\n",
    "#While numeric values can often be used without any feature\n",
    "#engineering, there will be cases when some form of manipulation\n",
    "#can be useful. For example on some occasions, you might not care\n",
    "#about the magnitude of a value but only care about its direction,\n",
    "#or if it exists at all. In these situations, you will want to\n",
    "#binarize a column. In the so_survey_df data, you have a large\n",
    "#number of survey respondents that are working voluntarily (without\n",
    "#pay). You will create a new column titled Paid_Job indicating\n",
    "#whether each person is paid (their salary is greater than zero).\n",
    "\n",
    "# Create the Paid_Job column filled with zeros\n",
    "#so_survey_df['Paid_Job'] = 0\n",
    "\n",
    "# Replace all the Paid_Job values where ConvertedSalary is > 0\n",
    "#so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\n",
    "\n",
    "# Print the first five rows of the columns\n",
    "#print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Paid_Job  ConvertedSalary\n",
    "#    0         0              0.0\n",
    "#    1         1          70841.0\n",
    "#    2         0              0.0\n",
    "#    3         1          21426.0\n",
    "#    4         1          41671.0\n",
    "#################################################\n",
    "#binarizing columns can also be useful for your target variables."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Binning values\n",
    "\n",
    "#For many continuous values you will care less about the exact value\n",
    "#of a numeric column, but instead care about the bucket it falls into.\n",
    "#This can be useful when plotting values, or simplifying your machine\n",
    "#learning models. It is mostly used on continuous variables where\n",
    "#accuracy is not the biggest concern e.g. age, height, wages.\n",
    "\n",
    "#Bins are created using pd.cut(df['column_name'], bins) where bins can\n",
    "#be an integer specifying the number of evenly spaced bins, or a list\n",
    "#of bin boundaries.\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary into 5 bins\n",
    "#so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'], 5)\n",
    "\n",
    "# Print the first 5 rows of the equal_binned column\n",
    "#print(so_survey_df[['equal_binned', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#              equal_binned  ConvertedSalary\n",
    "#    0  (-2000.0, 400000.0]              0.0\n",
    "#    1  (-2000.0, 400000.0]          70841.0\n",
    "#    2  (-2000.0, 400000.0]              0.0\n",
    "#    3  (-2000.0, 400000.0]          21426.0\n",
    "#    4  (-2000.0, 400000.0]          41671.0\n",
    "#################################################\n",
    "\n",
    "# Import numpy\n",
    "#import numpy as np\n",
    "\n",
    "# Specify the boundaries of the bins\n",
    "#bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
    "\n",
    "# Bin labels\n",
    "#labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary using these boundaries\n",
    "#so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'],\n",
    "#                                         bins, labels = labels)\n",
    "\n",
    "# Print the first 5 rows of the boundary_binned column\n",
    "#print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#      boundary_binned  ConvertedSalary\n",
    "#    0        Very low              0.0\n",
    "#    1          Medium          70841.0\n",
    "#    2        Very low              0.0\n",
    "#    3             Low          21426.0\n",
    "#    4             Low          41671.0\n",
    "#################################################\n",
    "#now you can bin columns with equal spacing and predefined boundaries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Why do missing values exist?**\n",
    "___\n",
    "- How gaps in data occur\n",
    "    - data not being collected properly\n",
    "    - collection and management errors\n",
    "    - data intentionally being omitted\n",
    "    - could be created due to transformations of the data\n",
    "- Why we care?\n",
    "    - some models cannot work with missing data (Nulls/NaNs)\n",
    "    - missing data may be a sign of a wider data issue\n",
    "    - missing data can be a useful feature\n",
    "- pd.info()\n",
    "- pd.isnull()\n",
    "- pd.isnull().sum()\n",
    "- df.notnull()\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#How sparse is my data?\n",
    "#Most data sets contain missing values, often represented as NaN\n",
    "#(Not a Number). If you are working with Pandas you can easily check\n",
    "#how many missing values exist in each column.\n",
    "\n",
    "#Let's find out how many of the developers taking the survey chose to\n",
    "#enter their age (found in the Age column of so_survey_df) and their\n",
    "#gender (Gender column of so_survey_df).\n",
    "\n",
    "# Subset the DataFrame\n",
    "#sub_df = so_survey_df[['Age', 'Gender']]\n",
    "\n",
    "# Print the number of non-missing values\n",
    "#print(sub_df.info())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    <class 'pandas.core.frame.DataFrame'>\n",
    "#    RangeIndex: 999 entries, 0 to 998\n",
    "#    Data columns (total 2 columns):\n",
    "#    Age       999 non-null int64\n",
    "#    Gender    693 non-null object\n",
    "#    dtypes: int64(1), object(1)\n",
    "#    memory usage: 15.7+ KB\n",
    "#    None\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Finding the missing values\n",
    "\n",
    "#While having a summary of how much of your data is missing can be\n",
    "#useful, often you will need to find the exact locations of these\n",
    "#missing values. Using the same subset of the StackOverflow data\n",
    "#from the last exercise (sub_df), you will show how a value can be\n",
    "#flagged as missing.\n",
    "\n",
    "# Print the top 10 entries of the DataFrame\n",
    "#print(sub_df.head(10))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Age  Gender\n",
    "#    0   21    Male\n",
    "#    1   38    Male\n",
    "#    2   45     NaN\n",
    "#    3   46    Male\n",
    "#    4   39    Male\n",
    "#    5   39    Male\n",
    "#    6   34    Male\n",
    "#    7   24  Female\n",
    "#    8   23    Male\n",
    "#    9   36     NaN\n",
    "#################################################\n",
    "\n",
    "# Print the locations of the missing values\n",
    "#print(sub_df.head(10).isnull())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#         Age  Gender\n",
    "#    0  False   False\n",
    "#    1  False   False\n",
    "#    2  False    True\n",
    "#    3  False   False\n",
    "#    4  False   False\n",
    "#    5  False   False\n",
    "#    6  False   False\n",
    "#    7  False   False\n",
    "#    8  False   False\n",
    "#    9  False    True\n",
    "#################################################\n",
    "\n",
    "# Print the locations of the non-missing values\n",
    "#print(sub_df.head(10).notnull())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#        Age  Gender\n",
    "#    0  True    True\n",
    "#    1  True    True\n",
    "#    2  True   False\n",
    "#    3  True    True\n",
    "#    4  True    True\n",
    "#    5  True    True\n",
    "#    6  True    True\n",
    "#    7  True    True\n",
    "#    8  True    True\n",
    "#    9  True   False\n",
    "#################################################\n",
    "# finding where the missing values exist can often be important."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dealing with missing values (I)**\n",
    "___\n",
    "- pd.dropna()\n",
    "- pd.drop()\n",
    "- random omissions\n",
    "    - complete case analysis / listwise deletion\n",
    "    - drawbacks\n",
    "        - deletes valid data points as well\n",
    "        - relies on randomness\n",
    "        - reduces information if a feature is removed (degrees of freedom)\n",
    "- replacement\n",
    "- recording missing values\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Listwise deletion\n",
    "\n",
    "#The simplest way to deal with missing values in your dataset when\n",
    "#they are occurring entirely at random is to remove those rows, also\n",
    "#called 'listwise deletion'.\n",
    "\n",
    "#Depending on the use case, you will sometimes want to remove all\n",
    "#missing values in your data while other times you may want to only\n",
    "#remove a particular column if too many values are missing in that\n",
    "#column.\n",
    "\n",
    "# Print the number of rows and columns\n",
    "#print(so_survey_df.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (999, 11)\n",
    "#################################################\n",
    "\n",
    "# Create a new DataFrame dropping all incomplete rows\n",
    "#no_missing_values_rows = so_survey_df.dropna()\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "#print(no_missing_values_rows.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (264, 11)\n",
    "#################################################\n",
    "\n",
    "# Create a new DataFrame dropping all columns with incomplete rows\n",
    "#no_missing_values_cols = so_survey_df.dropna(how='any', axis=1)\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "#print(no_missing_values_cols.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (999, 7)\n",
    "#################################################\n",
    "\n",
    "# Drop all rows where Gender is missing\n",
    "#no_gender = so_survey_df.dropna(subset=['Gender'])\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "#print(no_gender.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (693, 11)\n",
    "#################################################\n",
    "#as you can see dropping all rows that contain any missing values\n",
    "#may greatly reduce the size of your dataset. So you need to think\n",
    "#carefully and consider several trade-offs when deleting missing\n",
    "#values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Replacing missing values with constants\n",
    "\n",
    "#While removing missing data entirely maybe a correct approach in\n",
    "#many situations, this may result in a lot of information being\n",
    "#omitted from your models.\n",
    "\n",
    "#You may find categorical columns where the missing value is a valid\n",
    "#piece of information in itself, such as someone refusing to answer\n",
    "#a question in a survey. In these cases, you can fill all missing\n",
    "#values with a new category entirely, for example 'No response given'.\n",
    "\n",
    "# Print the count of occurrences\n",
    "#print(so_survey_df['Gender'].value_counts())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Male                                                                        632\n",
    "#    Female                                                                 53\n",
    "#    Female;Male                                                                 2\n",
    "#    Transgender                                                               2\n",
    "#    Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming      1\n",
    "#    Female;Transgender                                                             1\n",
    "#    Male;Non-binary. genderqueer. or gender non-conforming                         1\n",
    "#    Non-binary. genderqueer. or gender non-conforming                              1\n",
    "#    Name: Gender, dtype: int64\n",
    "#################################################\n",
    "\n",
    "# Replace missing values\n",
    "#so_survey_df['Gender'].fillna(value='Not Given', inplace=True)\n",
    "\n",
    "# Print the count of each value\n",
    "#print(so_survey_df['Gender'].value_counts())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Male                                                                         632\n",
    "#    Not Given                                                                    306\n",
    "#    Female                                                                        53\n",
    "#    Female;Male                                                                    2\n",
    "#    Transgender                                                                    2\n",
    "#    Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming      1\n",
    "#    Female;Transgender                                                             1\n",
    "#    Male;Non-binary. genderqueer. or gender non-conforming                         1\n",
    "#    Non-binary. genderqueer. or gender non-conforming                              1\n",
    "#    Name: Gender, dtype: int64\n",
    "#################################################\n",
    "#By filling in these missing values you can use the columns in your\n",
    "#analyses."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dealing with missing values (II)**\n",
    "___\n",
    "- If you cannot drop rows, what else can you do?\n",
    "    - **Categorical columns**: replace missing values with the most common occurring value or with a string that flags missing values such as 'None'\n",
    "    - **Numeric columns**: replace missing values with a suitable value\n",
    "        - measure of central tendency, e.g., mean, median\n",
    "- impute values based on the train set to both train and test sets.\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Filling continuous missing values\n",
    "\n",
    "#In the last lesson, you dealt with different methods of removing\n",
    "#data missing values and filling in missing values with a fixed\n",
    "#string. These approaches are valid in many cases, particularly when\n",
    "#dealing with categorical columns but have limited use when working\n",
    "#with continuous values. In these cases, it may be most valid to fill\n",
    "#the missing values in the column with a value calculated from the\n",
    "#entries present in the column.\n",
    "\n",
    "# Print the first five rows of StackOverflowJobsRecommend column\n",
    "#print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    NaN\n",
    "#    1    7.0\n",
    "#    2    8.0\n",
    "#    3    NaN\n",
    "#    4    8.0\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################\n",
    "\n",
    "# Fill missing values with the mean\n",
    "#so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
    "\n",
    "# Print the first five rows of StackOverflowJobsRecommend column\n",
    "#print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    7.061602\n",
    "#    1    7.000000\n",
    "#    2    8.000000\n",
    "#    3    7.061602\n",
    "#    4    8.000000\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################\n",
    "\n",
    "# Round the StackOverflowJobsRecommend values\n",
    "#so_survey_df['StackOverflowJobsRecommend'] = np.round (so_survey_df['StackOverflowJobsRecommend'])\n",
    "\n",
    "# Print the top 5 rows\n",
    "#print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    7.0\n",
    "#    1    7.0\n",
    "#    2    8.0\n",
    "#    3    7.0\n",
    "#    4    8.0\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################\n",
    "#remember you should only round your values if you are certain it is applicable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dealing with other data issues**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dealing with stray characters (I)\n",
    "\n",
    "#In this exercise, you will work with the RawSalary column of\n",
    "#so_survey_df which contains the wages of the respondents along with\n",
    "#the currency symbols and commas, such as $42,000. When importing\n",
    "#data from Microsoft Excel, more often than not you will come across\n",
    "#data in this form.\n",
    "\n",
    "# Remove the commas in the column\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace(',', '')\n",
    "\n",
    "# Remove the dollar signs in the column\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('$', '')\n",
    "\n",
    "#################################################\n",
    "#Replacing/removing specific characters is a very useful skill."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dealing with stray characters (II)\n",
    "\n",
    "#In the last exercise, you could tell quickly based off of the\n",
    "#df.head() call which characters were causing an issue. In many cases\n",
    "#this will not be so apparent. There will often be values deep within\n",
    "#a column that are preventing you from casting a column as a numeric\n",
    "#type so that it can be used in a model or further feature engineering.\n",
    "\n",
    "#One approach to finding these values is to force the column to the\n",
    "#data type desired using pd.to_numeric(), coercing any values causing\n",
    "#issues to NaN, Then filtering the DataFrame by just the rows containing\n",
    "#the NaN values.\n",
    "\n",
    "#Try to cast the RawSalary column as a float and it will fail as an\n",
    "#additional character can now be found in it. Find the character and\n",
    "#remove it so the column can be cast as a float.\n",
    "\n",
    "# Attempt to convert the column to numeric values\n",
    "#numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')\n",
    "\n",
    "# Find the indexes of missing values\n",
    "#idx = numeric_vals.isna()\n",
    "\n",
    "# Print the relevant rows\n",
    "#print(so_survey_df['RawSalary'][idx])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0             NaN\n",
    "#    2             NaN\n",
    "#    4       £41671.00\n",
    "#    6             NaN\n",
    "#    8             NaN\n",
    "#    ...\n",
    "#    49      £19500.00\n",
    "#    50            NaN\n",
    "#    52            NaN\n",
    "#    53      £36000.00\n",
    "#    54            NaN\n",
    "#    Name: RawSalary, Length: 401, dtype: object\n",
    "#################################################\n",
    "\n",
    "# Replace the offending characters\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('£', '')\n",
    "\n",
    "# Convert the column to float\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary'].astype('float')\n",
    "\n",
    "# Print the column\n",
    "#print(so_survey_df['RawSalary'])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0            NaN\n",
    "#    1        70841.0\n",
    "#    2            NaN\n",
    "#    3        21426.0\n",
    "#    4        41671.0\n",
    "#    ...\n",
    "#    994          NaN\n",
    "#    995      58746.0\n",
    "#    996      55000.0\n",
    "#    997          NaN\n",
    "#    998    1000000.0\n",
    "#    Name: RawSalary, Length: 999, dtype: float64\n",
    "#################################################\n",
    "#Remember that even after removing all the relevant characters, you\n",
    "#still need to change the type of the column to numeric if you want\n",
    "#to plot these continuous values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Method chaining\n",
    "\n",
    "#When applying multiple operations on the same column (like in the\n",
    "#previous exercises), you made the changes in several steps, assigning\n",
    "#the results back in each step. However, when applying multiple\n",
    "#successive operations on the same column, you can \"chain\" these\n",
    "#operations together for clarity and ease of management. This can be\n",
    "#achieved by calling multiple methods sequentially:\n",
    "\n",
    "# Method chaining\n",
    "#df['column'] = df['column'].method1().method2().method3()\n",
    "\n",
    "# Same as\n",
    "#df['column'] = df['column'].method1()\n",
    "#df['column'] = df['column'].method2()\n",
    "#df['column'] = df['column'].method3()\n",
    "\n",
    "#In this exercise you will repeat the steps you performed in the last\n",
    "#two exercises, but do so using method chaining.\n",
    "\n",
    "# Use method chaining\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary']\\\n",
    "#                              .str.replace(',', '')\\\n",
    "#                              .str.replace('$', '')\\\n",
    "#                              .str.replace('£', '')\\\n",
    "#                              .astype('float')\n",
    "\n",
    "# Print the RawSalary column\n",
    "#print(so_survey_df['RawSalary'])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0            NaN\n",
    "#    1        70841.0\n",
    "#    2            NaN\n",
    "#    3        21426.0\n",
    "#    4        41671.0\n",
    "#    ...\n",
    "#    994          NaN\n",
    "#    995      58746.0\n",
    "#    996      55000.0\n",
    "#    997          NaN\n",
    "#    998    1000000.0\n",
    "#    Name: RawSalary, Length: 999, dtype: float64\n",
    "#################################################\n",
    "#Custom functions can be also used when method chaining using the\n",
    "#.apply() method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data distributions**\n",
    "___\n",
    "- most models assume your data is normally distributed and/or on the same scale\n",
    "    - 1 sd = 66.27%; 2 sd = 95.45%; 3 sd = 99.73%\n",
    "- decision tree-based models do not make this assumption\n",
    "    - As decision trees split along a singular point, they do not require all the columns to be on the same scale.\n",
    "- delving deeper with box plots\n",
    "    - Interquartile Range (IQR) = 25th (Q1) percentile to 75th (Q3) percentile\n",
    "    - Minimum = Q1 - 1.5 IQR\n",
    "    - Maximum = Q3 + 1.5 IQR\n",
    "    - outliers are outside Minimum or Maximum\n",
    "- pairing distributions with seaborn library\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#What does your data look like? (I)\n",
    "\n",
    "#Up until now you have focused on creating new features and dealing\n",
    "#with issues in your data. Feature engineering can also be used to\n",
    "#make the most out of the data that you already have and use it more\n",
    "#effectively when creating machine learning models.\n",
    "\n",
    "#Many algorithms may assume that your data is normally distributed,\n",
    "#or at least that all your columns are on the same scale. This will\n",
    "#often not be the case, e.g. one feature may be measured in thousands\n",
    "#of dollars while another would be number of years. In this exercise,\n",
    "#you will create plots to examine the distributions of some numeric\n",
    "#columns in the so_survey_df DataFrame, stored in so_numeric_df.\n",
    "\n",
    "# Create a histogram\n",
    "#so_numeric_df.hist()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.1.svg](_images/16.1.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a boxplot of two columns\n",
    "#so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.2.svg](_images/16.2.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a boxplot of ConvertedSalary\n",
    "#so_numeric_df[['ConvertedSalary']].boxplot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.3.svg](_images/16.3.svg)\n",
    "as you can see the distributions of columns in a dataset can vary quite a bit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#What does your data look like? (II)\n",
    "\n",
    "#In the previous exercise you looked at the distribution of individual\n",
    "#columns. While this is a good start, a more detailed view of how\n",
    "#different features interact with each other may be useful as this\n",
    "#can impact your decision on what to transform and how.\n",
    "\n",
    "# Import packages\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# Plot pairwise relationships\n",
    "#sns.pairplot(so_numeric_df)\n",
    "\n",
    "# Show plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.4.svg](_images/16.4.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "#print(so_numeric_df.describe())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#           ConvertedSalary         Age  Years Experience\n",
    "#    count     9.990000e+02  999.000000        999.000000\n",
    "#    mean      6.161746e+04   36.003003          9.961962\n",
    "#    std       1.760924e+05   13.255127          4.878129\n",
    "#    min       0.000000e+00   18.000000          0.000000\n",
    "#    25%       0.000000e+00   25.000000          7.000000\n",
    "#    50%       2.712000e+04   35.000000         10.000000\n",
    "#    75%       7.000000e+04   45.000000         13.000000\n",
    "#    max       2.000000e+06   83.000000         27.000000\n",
    "#################################################\n",
    "#understanding these summary statistics of a column can be very\n",
    "#valuable when deciding what transformations are necessary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scaling and transformations**\n",
    "___\n",
    "- Min-Max scaling / Normalization\n",
    "    - distribution remains the same\n",
    "    - values change to range 0-1\n",
    "    - MinMaxScaler() from scikit-learn preprocessing module\n",
    "- Standardization\n",
    "    - centers distribution around the mean = zero\n",
    "    - StandardScaler() from scikit-learn preprocessing module\n",
    "- Log Transformation\n",
    "    - can make highly skewed distributions less skewed\n",
    "    - PowerTransformer() from scikit-learn preprocessing module\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "#As discussed in the video, in normalization you linearly scale the\n",
    "#entire column between 0 and 1, with 0 corresponding with the lowest\n",
    "#value in the column, and 1 with the largest.\n",
    "\n",
    "#When using scikit-learn (the most commonly used machine learning\n",
    "#library in Python) you can use a MinMaxScaler to apply normalization.\n",
    "#(It is called this as it scales your values between a minimum and\n",
    "#maximum value.)\n",
    "\n",
    "# Import MinMaxScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate MinMaxScaler\n",
    "#MM_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit MM_scaler to the data\n",
    "#MM_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "#so_numeric_df['Age_MM'] = MM_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "#print(so_numeric_df[['Age_MM', 'Age']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#         Age_MM  Age\n",
    "#    0  0.046154   21\n",
    "#    1  0.307692   38\n",
    "#    2  0.415385   45\n",
    "#    3  0.430769   46\n",
    "#    4  0.323077   39\n",
    "#################################################\n",
    "#Did you notice that all values have been scaled between 0 and 1?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Standardization\n",
    "\n",
    "#While normalization can be useful for scaling a column between two\n",
    "#data points, it is hard to compare two scaled columns if even one of\n",
    "#them is overly affected by outliers. One commonly used solution to\n",
    "#this is called standardization, where instead of having a strict\n",
    "#upper and lower bound, you center the data around its mean, and\n",
    "#calculate the number of standard deviations away from mean each data\n",
    "#point is.\n",
    "\n",
    "# Import StandardScaler\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "#SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit SS_scaler to the data\n",
    "#SS_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "#so_numeric_df['Age_SS'] = SS_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "#print(so_numeric_df[['Age_SS', 'Age']].head())\n",
    "\n",
    "#################################################\n",
    "#       Age_SS  Age\n",
    "#    0 -1.132431   21\n",
    "#    1  0.150734   38\n",
    "#    2  0.679096   45\n",
    "#    3  0.754576   46\n",
    "#    4  0.226214   39\n",
    "#################################################\n",
    "#you can see that the values have been scaled linearly, but not\n",
    "#between set values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Log transformation\n",
    "\n",
    "#In the previous exercises you scaled the data linearly, which will\n",
    "#not affect the data's shape. This works great if your data is\n",
    "#normally distributed (or closely normally distributed), an assumption\n",
    "#that a lot of machine learning models make. Sometimes you will work\n",
    "#with data that closely conforms to normality, e.g the height or\n",
    "#weight of a population. On the other hand, many variables in the\n",
    "#real world do not follow this pattern e.g, wages or age of a\n",
    "#population. In this exercise you will use a log transform on the\n",
    "#ConvertedSalary column in the so_numeric_df DataFrame as it has a\n",
    "#large amount of its data centered around the lower values, but\n",
    "#contains very high values also. These distributions are said to have\n",
    "#a long right tail.\n",
    "\n",
    "# Import PowerTransformer\n",
    "#from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Instantiate PowerTransformer\n",
    "#pow_trans = PowerTransformer()\n",
    "\n",
    "# Train the transform on the data\n",
    "#pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Apply the power transform to the data\n",
    "#so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Plot the data before and after the transformation\n",
    "#so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.5.svg](_images/16.5.svg)\n",
    "Did you notice the change in the shape of the distribution?\n",
    "ConvertedSalary_LG column looks much more normal than the original\n",
    "ConvertedSalary column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Removing outliers**\n",
    "___\n",
    "- Quantile based detection\n",
    "- Standard deviation based detection\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Percentage based outlier removal\n",
    "\n",
    "#One way to ensure a small portion of data is not having an overly\n",
    "#adverse effect is by removing a certain percentage of the largest\n",
    "#and/or smallest values in the column. This can be achieved by\n",
    "#finding the relevant quantile and trimming the data using it with\n",
    "#a mask. This approach is particularly useful if you are concerned\n",
    "#that the highest values in your dataset should be avoided. When\n",
    "#using this approach, you must remember that even if there are no\n",
    "#outliers, this will still remove the same top N percentage from the\n",
    "#dataset.\n",
    "\n",
    "# Find the 95th quantile\n",
    "#quantile = so_numeric_df['ConvertedSalary'].quantile(0.95)\n",
    "\n",
    "# Trim the outliers\n",
    "#trimmed_df = so_numeric_df[so_numeric_df['ConvertedSalary'] < quantile]\n",
    "\n",
    "# The original histogram\n",
    "#so_numeric_df[['ConvertedSalary']].hist()\n",
    "#plt.show()\n",
    "#plt.clf()\n",
    "\n",
    "# The trimmed histogram\n",
    "#trimmed_df[['ConvertedSalary']].hist()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.6.svg](_images/16.6.svg)\n",
    "![_images/16.7.svg](_images/16.7.svg)\n",
    "In the next exercise, you will work with a more statistically sound\n",
    "approach in removing outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Statistical outlier removal\n",
    "\n",
    "#While removing the top N% of your data is useful for ensuring that\n",
    "#very spurious points are removed, it does have the disadvantage of\n",
    "#always removing the same proportion of points, even if the data is\n",
    "#correct. A commonly used alternative approach is to remove data that\n",
    "#sits further than three standard deviations from the mean. You can\n",
    "#implement this by first calculating the mean and standard deviation\n",
    "#of the relevant column to find upper and lower bounds, and applying\n",
    "#these bounds as a mask to the DataFrame. This method ensures that\n",
    "#only data that is genuinely different from the rest is removed, and\n",
    "#will remove fewer points if the data is close together."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.3.svg](_images/16.3.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the mean and standard dev\n",
    "#std = so_numeric_df['ConvertedSalary'].std()\n",
    "#mean = so_numeric_df['ConvertedSalary'].mean()\n",
    "\n",
    "# Calculate the cutoff\n",
    "#cut_off = std * 3\n",
    "#lower, upper = mean - cut_off, mean + cut_off\n",
    "\n",
    "# Trim the outliers\n",
    "#trimmed_df = so_numeric_df[(so_numeric_df['ConvertedSalary'] < upper) \\\n",
    "#                           & (so_numeric_df['ConvertedSalary'] > lower)]\n",
    "\n",
    "# The trimmed box plot\n",
    "#trimmed_df[['ConvertedSalary']].boxplot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.8.svg](_images/16.8.svg)\n",
    "Did you notice the scale change on the y-axis?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scaling and transforming new data**\n",
    "___\n",
    "- you fit and transform the training data, but only transform test data\n",
    "- remove outliers from training set, but generally not on test test\n",
    "- Why only use training data?\n",
    "    - **data leakage** - using data that you won't have access to when assessing the performance of your model\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train and testing transformations (I)\n",
    "\n",
    "#So far you have created scalers based on a column, and then applied\n",
    "#the scaler to the same data that it was trained on. When creating\n",
    "#machine learning models you will generally build your models on\n",
    "#historic data (train set) and apply your model to new unseen data\n",
    "#(test set). In these cases you will need to ensure that the same\n",
    "#scaling is being applied to both the training and test data.\n",
    "\n",
    "#To do this in practice you train the scaler on the train set, and\n",
    "#keep the trained scaler to apply it to the test set. You should\n",
    "#never retrain a scaler on the test set.\n",
    "\n",
    "#For this exercise and the next, we split the so_numeric_df\n",
    "#DataFrame into train (so_train_numeric) and test (so_test_numeric)\n",
    "#sets.\n",
    "\n",
    "# Import StandardScaler\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply a standard scaler to the data\n",
    "#SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit the standard scaler to the data\n",
    "#SS_scaler.fit(so_train_numeric[['Age']])\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "#so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])\n",
    "#print(so_test_numeric[['Age', 'Age_ss']].head())\n",
    "\n",
    "#################################################\n",
    "#       Age    Age_ss\n",
    "#    700   35 -0.069265\n",
    "#    701   18 -1.343218\n",
    "#    702   47  0.829997\n",
    "#    703   57  1.579381\n",
    "#    704   41  0.380366\n",
    "#################################################\n",
    "#Data leakage is one of the most common mistakes data scientists tend\n",
    "#to make, and I hope that you won't!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train and testing transformations (II)\n",
    "\n",
    "#Similar to applying the same scaler to both your training and test\n",
    "#sets, if you have removed outliers from the train set, you probably\n",
    "#want to do the same on the test set as well. Once again you should\n",
    "#ensure that you use the thresholds calculated only from the train\n",
    "#set to remove outliers from the test set.\n",
    "\n",
    "#Similar to the last exercise, we split the so_numeric_df DataFrame\n",
    "#into train (so_train_numeric) and test (so_test_numeric) sets.\n",
    "\n",
    "#train_std = so_train_numeric['ConvertedSalary'].std()\n",
    "#train_mean = so_train_numeric['ConvertedSalary'].mean()\n",
    "\n",
    "#cut_off = train_std * 3\n",
    "#train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n",
    "\n",
    "# Trim the test DataFrame\n",
    "#trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) \\\n",
    "#                             & (so_test_numeric['ConvertedSalary'] > train_lower)]\n",
    "\n",
    "#################################################\n",
    "#In the next chapter, you will deal with unstructured (text) data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Encoding text**\n",
    "___\n",
    "- Standardizing your text\n",
    "    - remove unwanted characters using str.replace() and regular expressions\n",
    "        - [a-zA-Z]: all letter characters\n",
    "        - [^a-zA-Z]: all non letter characters\n",
    "- Standardize the case\n",
    "    - str.lower()\n",
    "- length of text\n",
    "    - .len()\n",
    "- word count\n",
    "- average length of word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Cleaning up your text\n",
    "\n",
    "#Unstructured text data cannot be directly used in most analyses.\n",
    "#Multiple steps need to be taken to go from a long free form string\n",
    "#to a set of numeric columns in the right format that can be\n",
    "#ingested by a machine learning model. The first step of this process\n",
    "#is to standardize the data and eliminate any characters that could\n",
    "#cause problems later on in your analytic pipeline.\n",
    "\n",
    "#In this chapter you will be working with a new dataset containing\n",
    "#the inaugural speeches of the presidents of the United States\n",
    "#loaded as speech_df, with the speeches stored in the text column.\n",
    "\n",
    "# Print the first 5 rows of the text column\n",
    "#print(speech_df['text'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    Fellow-Citizens of the Senate and of the House...\n",
    "#    1    Fellow Citizens:  I AM again called upon by th...\n",
    "#    2    WHEN it was first perceived, in early times, t...\n",
    "#    3    Friends and Fellow-Citizens:  CALLED upon to u...\n",
    "#    4    PROCEEDING, fellow-citizens, to that qualifica...\n",
    "#    Name: text, dtype: object\n",
    "#################################################\n",
    "\n",
    "# Replace all non letter characters with a whitespace\n",
    "#speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
    "\n",
    "# Change to lower case\n",
    "#speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "\n",
    "# Print the first 5 rows of the text_clean column\n",
    "#print(speech_df['text_clean'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    fellow citizens of the senate and of the house...\n",
    "#    1    fellow citizens   i am again called upon by th...\n",
    "#    2    when it was first perceived  in early times  t...\n",
    "#    3    friends and fellow citizens   called upon to u...\n",
    "#    4    proceeding  fellow citizens  to that qualifica...\n",
    "#    Name: text_clean, dtype: object\n",
    "#################################################\n",
    "#now your text strings have been standardized and cleaned up. You\n",
    "#can now use this new column (text_clean) to extract information\n",
    "#about the speeches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#High level text features\n",
    "\n",
    "#Once the text has been cleaned and standardized you can begin\n",
    "#creating features from the data. The most fundamental information\n",
    "#you can calculate about free form text is its size, such as its\n",
    "#length and number of words. In this exercise (and the rest of this\n",
    "#chapter), you will focus on the cleaned/transformed text column\n",
    "#(text_clean) you created in the last exercise.\n",
    "\n",
    "# Find the length of each text\n",
    "#speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
    "\n",
    "# Count the number of words in each text\n",
    "#speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
    "\n",
    "# Find the average length of word\n",
    "#speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Print the first 5 rows of these columns\n",
    "#print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                                               text_clean  char_cnt  word_cnt  avg_word_length\n",
    "#    0   fellow citizens of the senate and of the house...      8616      1432         6.016760\n",
    "#    1   fellow citizens   i am again called upon by th...       787       135         5.829630\n",
    "#    2   when it was first perceived  in early times  t...     13871      2323         5.971158\n",
    "#    3   friends and fellow citizens   called upon to u...     10144      1736         5.843318\n",
    "#    4   proceeding  fellow citizens  to that qualifica...     12902      2169         5.948363\n",
    "#################################################\n",
    "#These features may appear basic but can be quite useful in ML models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Word counts**\n",
    "___\n",
    "- text to columns\n",
    "    - one column per word with word counts for each word\n",
    "    - CountVectorizer in sklearn.feature_extraction.text\n",
    "        - min_df, max_df = 0.1, 0.9\n",
    "        - creates sparse array, convert to array using .toarray()\n",
    "            - to get feature names from the array .get_feature_names()\n",
    "        - combine into dataframe\n",
    "            - pd.DataFrame(cv_tranformed.toarray(), columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "        - updating/combining your dataframe\n",
    "            - pd.concat([list], axis=1, sort=False)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Counting words (I)\n",
    "\n",
    "#Once high level information has been recorded you can begin creating\n",
    "#features based on the actual content of each text. One way to do\n",
    "#this is to approach it in a similar way to how you worked with\n",
    "#categorical variables in the earlier lessons.\n",
    "\n",
    "#For each unique word in the dataset a column is created.\n",
    "\n",
    "#For each entry, the number of times this word occurs is counted and\n",
    "#the count value is entered into the respective column.\n",
    "\n",
    "#These \"count\" columns can then be used to train machine learning\n",
    "#models.\n",
    "\n",
    "# Import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "#cv = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer\n",
    "#cv.fit(speech_df['text_clean'])\n",
    "\n",
    "# Print feature names\n",
    "#print(cv.get_feature_names())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    ['abandon', 'abandoned', 'abandonment', 'abate', 'abdicated', 'abeyance', 'abhorring', 'abide', 'abiding', 'abilities', 'ability', 'abject', 'able', 'ably', 'abnormal', 'abode', 'abolish', 'abolished', 'abolishing', 'aboriginal', 'aborigines', 'abound', 'abounding', 'abounds', 'about', 'above', 'abraham', 'abreast', 'abridging',\n",
    "#################################################\n",
    "#this vectorizer can be applied to both the text it was trained on, and new texts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Counting words (II)\n",
    "\n",
    "#Once the vectorizer has been fit to the data, it can be used to\n",
    "#transform the text to an array representing the word counts. This\n",
    "#array will have a row per block of text and a column for each of\n",
    "#the features generated by the vectorizer that you observed in the\n",
    "#last exercise.\n",
    "\n",
    "#The vectorizer to you fit in the last exercise (cv) is available\n",
    "#in your workspace.\n",
    "\n",
    "# Apply the vectorizer\n",
    "#cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the full array\n",
    "#cv_array = cv_transformed.toarray()\n",
    "#print(cv_array)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [[0 0 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]\n",
    "#     [0 1 0 ... 0 0 0]\n",
    "#     ...\n",
    "#     [0 1 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]]\n",
    "#################################################\n",
    "\n",
    "# Print the shape of cv_array\n",
    "#print(cv_array.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (58, 9043)\n",
    "#################################################\n",
    "#The speeches have 9043 unique words, which is a lot! In the next\n",
    "#exercise, you will see how to create a limited set of features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Limiting your features\n",
    "\n",
    "#As you have seen, using the CountVectorizer with its default\n",
    "#settings creates a feature for every single word in your corpus.\n",
    "#This can create far too many features, often including ones that\n",
    "#will provide very little analytical value.\n",
    "\n",
    "#For this purpose CountVectorizer has parameters that you can set to\n",
    "#reduce the number of features:\n",
    "\n",
    "#min_df : Use only words that occur in more than this percentage of documents.\n",
    "#This can be used to remove outlier words that will not generalize across texts.\n",
    "\n",
    "#max_df : Use only words that occur in less than this percentage of documents.\n",
    "#This is useful to eliminate very common words that occur in every corpus without\n",
    "#adding value such as \"and\" or \"the\".\n",
    "\n",
    "# Import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Specify arguments to limit the number of features generated\n",
    "#cv = CountVectorizer(min_df=0.2, max_df=0.8)\n",
    "\n",
    "# Fit, transform, and convert into array\n",
    "#cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "#cv_array = cv_transformed.toarray()\n",
    "\n",
    "# Print the array shape\n",
    "#print(cv_array.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (58, 818)\n",
    "#################################################\n",
    "# the number of features (unique words) greatly reduced from 9043 to 818."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Text to DataFrame\n",
    "#Now that you have generated these count based features in an array\n",
    "#you will need to reformat them so that they can be combined with\n",
    "#the rest of the dataset. This can be achieved by converting the\n",
    "#array into a pandas DataFrame, with the feature names you found\n",
    "#earlier as the column names, and then concatenate it with the\n",
    "#original DataFrame.\n",
    "\n",
    "#The numpy array (cv_array) and the vectorizer (cv) you fit in the\n",
    "#last exercise are available in your workspace.\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "#cv_df = pd.DataFrame(cv_array,\n",
    "#                     columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "#speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "#print(speech_df_new.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                    Name         Inaugural Address                      Date                                               text                                         text_clean  ...  Counts_years  \\\n",
    "#    0  George Washington   First Inaugural Address  Thursday, April 30, 1789  Fellow-Citizens of the Senate and of the House...  fellow citizens of the senate and of the house...  ...             1\n",
    "#    1  George Washington  Second Inaugural Address     Monday, March 4, 1793  Fellow Citizens:  I AM again called upon by th...  fellow citizens   i am again called upon by th...  ...             0\n",
    "#    2         John Adams         Inaugural Address   Saturday, March 4, 1797  WHEN it was first perceived, in early times, t...  when it was first perceived  in early times  t...  ...             3\n",
    "#    3   Thomas Jefferson   First Inaugural Address  Wednesday, March 4, 1801  Friends and Fellow-Citizens:  CALLED upon to u...  friends and fellow citizens   called upon to u...  ...             0\n",
    "#    4   Thomas Jefferson  Second Inaugural Address     Monday, March 4, 1805  PROCEEDING, fellow-citizens, to that qualifica...  proceeding  fellow citizens  to that qualifica...  ...             2\n",
    "#\n",
    "#       Counts_yet  Counts_you  Counts_young  Counts_your\n",
    "#    0           0           5             0            9\n",
    "#    1           0           0             0            1\n",
    "#    2           0           0             0            1\n",
    "#    3           2           7             0            7\n",
    "#    4           2           4             0            4\n",
    "#\n",
    "#    [5 rows x 826 columns]\n",
    "#################################################\n",
    "#With the new features combined with the orginial DataFrame they can \n",
    "#be now used for ML models or analysis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}