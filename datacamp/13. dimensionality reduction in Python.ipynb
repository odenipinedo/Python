{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Introduction**\n",
    "___\n",
    "- Tidy data\n",
    "    - every column is a feature\n",
    "    - every row is an observation for each variable\n",
    "    - Pandas dataframe .shape attribute\n",
    "- high dimensionality > 10 columns\n",
    "- When to use dimensionality reduction?\n",
    "    - drop columns with no variance (i.e. same values)\n",
    "    - Pandas dataframe .describe() method\n",
    "        - no variance = std = 0, max and min are the same\n",
    "        - exclude = 'number' --> will show information for non-numeric values\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Removing features without variance\n",
    "\n",
    "#A sample of the Pokemon dataset has been loaded as pokemon_df. To\n",
    "#get an idea of which features have little variance you should use\n",
    "#the IPython Shell to calculate summary statistics on this sample.\n",
    "#Then adjust the code to create a smaller, easier to understand,\n",
    "#dataset.\n",
    "\n",
    "# Leave this list as is\n",
    "number_cols = ['HP', 'Attack', 'Defense']\n",
    "\n",
    "# Remove the feature without variance from this list\n",
    "non_number_cols = ['Name', 'Type']\n",
    "\n",
    "# Create a new dataframe by subselecting the chosen features\n",
    "#df_selected = pokemon_df[number_cols + non_number_cols]\n",
    "\n",
    "# Prints the first 5 lines of the new dataframe\n",
    "#print(df_selected.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       HP  Attack  Defense                   Name   Type\n",
    "#    0  45      49       49              Bulbasaur  Grass\n",
    "#    1  60      62       63                Ivysaur  Grass\n",
    "#    2  80      82       83               Venusaur  Grass\n",
    "#    3  80     100      123  VenusaurMega Venusaur  Grass\n",
    "#    4  39      52       43             Charmander   Fire\n",
    "#################################################\n",
    "#All Pokemon in this dataset are non-legendary and from generation\n",
    "#one so you could choose to drop those two features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature selection vs feature extraction**\n",
    "___\n",
    "- Why reduce dimensionality?\n",
    "    - your dataset will:\n",
    "        - be less complex\n",
    "        - require less disk space\n",
    "        - have lower chance of model overfitting\n",
    "- Feature selection\n",
    "    - .drop('column name', axis=1) [axis indicates column instead of row]\n",
    "- Building a pairplot\n",
    "    - sns.pairplot(data, hue='', diag_kind='hist')\n",
    "- Feature extraction\n",
    "    - calculating new feature(s) from original feature(s)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visually detecting redundant features\n",
    "#Data visualization is a crucial step in any data exploration. Let's\n",
    "#use Seaborn to explore some samples of the US Army ANSUR body\n",
    "#measurement dataset.\n",
    "\n",
    "#Two data samples have been pre-loaded as ansur_df_1 and ansur_df_2.\n",
    "\n",
    "#Seaborn has been imported as sns.\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(ansur_df_1, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.1.svg](_images/13.1.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Two features are basically duplicates, remove one of them from\n",
    "#the dataset.\n",
    "\n",
    "# Remove one of the redundant features\n",
    "#reduced_df = ansur_df_1.drop('stature_m', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(reduced_df, hue='Gender')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.2.svg](_images/13.2.svg)\n",
    "the body height (inches) and stature (meters) hold the same information in a different unit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Now create a pairplot of the ansur_df_2 data sample and color the\n",
    "#points using the 'Gender' feature.\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(ansur_df_2, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.3.svg](_images/13.3.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#One feature has no variance, remove it from the dataset.\n",
    "# Remove the redundant feature\n",
    "#reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.4.svg](_images/13.4.svg)\n",
    "all the individuals in the second sample have two legs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**t-SNE visualization of high-dimensional data**\n",
    "___\n",
    "- t-distributed stochastic neighbor embedding\n",
    "- t-SNE maximizes distance in 2-dimensional space between dimensions in higher dimensional space\n",
    "- does not work with non-numeric values\n",
    "- learning rate (10-1000) - lower number is conservative\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fitting t-SNE to the ANSUR data\n",
    "\n",
    "#t-SNE is a great technique for visual exploration of high dimensional\n",
    "#datasets. In this exercise, you'll apply it to the ANSUR dataset. You'll\n",
    "#remove non-numeric columns from the pre-loaded dataset df and fit TSNE to\n",
    "#this numeric dataset.\n",
    "\n",
    "# Non-numerical columns in the dataset\n",
    "#non_numeric = ['Branch', 'Gender', 'Component']\n",
    "\n",
    "# Drop the non-numerical columns from df\n",
    "#df_numeric = df.drop(non_numeric, axis=1)\n",
    "\n",
    "# Create a t-SNE model with learning rate 50\n",
    "#m = TSNE(learning_rate=50)\n",
    "\n",
    "# Fit and transform the t-SNE model on the numeric dataset\n",
    "#tsne_features = m.fit_transform(df_numeric)\n",
    "\n",
    "#################################################\n",
    "#t-SNE reduced the more than 90 features in the dataset to just 2\n",
    "#which you can now plot.\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#t-SNE visualisation of dimensionality\n",
    "#Time to look at the results of your hard work. In this exercise,\n",
    "#you will visualize the output of t-SNE dimensionality reduction on\n",
    "#the combined male and female Ansur dataset. You'll create 3\n",
    "#scatterplots of the 2 t-SNE features ('x' and 'y') which were\n",
    "#added to the dataset df. In each scatterplot you'll color the\n",
    "#points according to a different categorical variable.\n",
    "\n",
    "#seaborn has already been imported as sns and matplotlib.pyplot as\n",
    "#plt.\n",
    "\n",
    "# Color the points according to Army Component\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Component', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.5.svg](_images/13.5.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Color the points by Army Branch\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Branch', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.6.svg](_images/13.6.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Color the points by Gender\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Gender', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.7.svg](_images/13.7.svg)\n",
    "There is a Male and a Female cluster. t-SNE found these gender\n",
    "differences in body shape without being told about them explicitly!\n",
    "\n",
    "From the second plot you learned there are more males in the\n",
    "Combat Arms Branch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The curse of dimensionality**\n",
    "___\n",
    "- as number of features increase in order to better fit a model, the number of observations must increase exponentially\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train - test split\n",
    "#In this chapter, you will keep working with the ANSUR dataset.\n",
    "#Before you can build a model on your dataset, you should first\n",
    "#decide on which feature you want to predict. In this case, you're\n",
    "#trying to predict gender.\n",
    "\n",
    "#You need to extract the column holding this feature from the\n",
    "#dataset and then split the data into a training and test set. The\n",
    "#training set will be used to train the model and the test set will\n",
    "#be used to check its performance on unseen data.\n",
    "\n",
    "#ansur_df has been pre-loaded for you.\n",
    "\n",
    "# Import train_test_split()\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the Gender column as the feature to be predicted (y)\n",
    "#y = ansur_df['Gender']\n",
    "\n",
    "# Remove the Gender column to create the training data\n",
    "#X = ansur_df.drop('Gender', axis=1)\n",
    "\n",
    "# Perform a 70% train and 30% test data split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#print(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    300 rows in test set vs. 700 in training set. 91 Features.\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fitting and testing the model\n",
    "\n",
    "#In the previous exercise, you split the dataset into X_train,\n",
    "#X_test, y_train, and y_test. These datasets have been pre-loaded\n",
    "#for you. You'll now create a support vector machine classifier\n",
    "#model (SVC()) and fit that to the training data. You'll then\n",
    "#calculate the accuracy on both the test and training set to detect\n",
    "#overfitting.\n",
    "\n",
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "#svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "#svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "#accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "#accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "#print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    49.7% accuracy on test set vs. 100.0% on training set\n",
    "#################################################\n",
    "#Looks like the model badly overfits on the training data. On unseen\n",
    "#data it performs worse than a random selector would."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Accuracy after dimensionality reduction\n",
    "\n",
    "#You'll reduce the overfit with the help of dimensionality reduction.\n",
    "#In this case, you'll apply a rather drastic form of dimensionality\n",
    "#reduction by only selecting a single column that has some good\n",
    "#information to distinguish between genders. You'll repeat the\n",
    "#train-test split, model fit and prediction steps to compare the\n",
    "#accuracy on test vs. training data.\n",
    "\n",
    "#All relevant packages and y have been pre-loaded.\n",
    "\n",
    "# Assign just the 'neckcircumferencebase' column from ansur_df to X\n",
    "#X = ansur_df[['neckcircumferencebase']]\n",
    "\n",
    "# Split the data, instantiate a classifier and fit the data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "#svc = SVC()\n",
    "#svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "#accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "#accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "#print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#   93.3% accuracy on test set vs. 94.9% on training set\n",
    "#################################################\n",
    "#On the full dataset the model is rubbish but with a single feature\n",
    "#we can make good predictions? This is an example of the curse of\n",
    "#dimensionality! The model badly overfits when we feed it too many\n",
    "#features. It overlooks that neck circumference by itself is pretty\n",
    "#different for males and females."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Features with missing values or little variance**\n",
    "___\n",
    "- Variance thresholds are not always easy to interpret or compare between features\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Finding a good variance threshold\n",
    "\n",
    "#You'll be working on a slightly modified subsample of the ANSUR\n",
    "#dataset with just head measurements pre-loaded as head_df.\n",
    "\n",
    "#Create a boxplot on head_df.\n",
    "\n",
    "# Create the boxplot\n",
    "#head_df.boxplot()\n",
    "\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.8.svg](_images/13.8.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Normalize the data by dividing the dataframe with its mean values.\n",
    "\n",
    "# Normalize the data\n",
    "#normalized_df = head_df / head_df.mean()\n",
    "\n",
    "#normalized_df.boxplot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.9.svg](_images/13.9.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Print the variances of the normalized data.\n",
    "\n",
    "# Normalize the data\n",
    "#normalized_df = head_df / head_df.mean()\n",
    "\n",
    "# Print the variances of the normalized data\n",
    "#print(normalized_df.var())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    headbreadth          1.678952e-03\n",
    "#    headcircumference    1.029623e-03\n",
    "#    headlength           1.867872e-03\n",
    "#    tragiontopofhead     2.639840e-03\n",
    "#    n_hairs              1.002552e-08\n",
    "#    measurement_error    3.231707e-27\n",
    "#    dtype: float64\n",
    "#################################################\n",
    "#Q: If you want to remove the 2 very low variance features.\n",
    "#What would be a good variance threshold?\n",
    "#A: 1.0e-03"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Features with low variance\n",
    "\n",
    "#In the previous exercise you established that 0.001 is a good\n",
    "#threshold to filter out low variance features in head_df after\n",
    "#normalization. Now use the VarianceThreshold feature selector to\n",
    "#remove these features.\n",
    "\n",
    "#from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "#sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "#sel.fit(head_df / head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "#mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "#reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "#print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Dimensionality reduced from 6 to 4.\n",
    "#################################################\n",
    "#you've successfully removed the 2 low-variance features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Removing features with many missing values\n",
    "\n",
    "#You'll apply feature selection on the Boston Public Schools\n",
    "#dataset which has been pre-loaded as school_df. Calculate the\n",
    "#missing value ratio per feature and then create a mask to remove\n",
    "#features with many missing values.\n",
    "\n",
    "#school_df.isna().sum() / len(school_df)\n",
    "#################################################\n",
    "#x             0.000000\n",
    "#y             0.000000\n",
    "#objectid_1    0.000000\n",
    "#objectid      0.000000\n",
    "#bldg_id       0.000000\n",
    "#bldg_name     0.000000\n",
    "#address       0.000000\n",
    "#city          0.000000\n",
    "#zipcode       0.000000\n",
    "#csp_sch_id    0.000000\n",
    "#sch_id        0.000000\n",
    "#sch_name      0.000000\n",
    "#sch_label     0.000000\n",
    "#sch_type      0.000000\n",
    "#shared        0.877863\n",
    "#complex       0.984733\n",
    "#label         0.000000\n",
    "#tlt           0.000000\n",
    "#pl            0.000000\n",
    "#point_x       0.000000\n",
    "#point_y       0.000000\n",
    "#dtype: float64\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create a boolean mask on whether each feature has less than 50%\n",
    "#missing values.\n",
    "\n",
    "#Apply the mask to school_df to select columns without many missing\n",
    "#values.\n",
    "\n",
    "# Create a boolean mask on whether each feature less than 50% missing values.\n",
    "#mask = school_df.isna().sum() / len(school_df) < 0.5\n",
    "\n",
    "# Create a reduced dataset by applying the mask\n",
    "#reduced_df = school_df.loc[:, mask]\n",
    "\n",
    "#print(school_df.shape)\n",
    "#print(reduced_df.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (131, 21)\n",
    "#    (131, 19)\n",
    "#################################################\n",
    "#The number of features went down from 21 to 19."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pairwise correlation**\n",
    "___\n",
    "- pairplots\n",
    "- correlation coefficient\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing the correlation matrix\n",
    "\n",
    "#Reading the correlation matrix of ansur_df in its raw, numeric\n",
    "#format doesn't allow us to get a quick overview. Let's improve\n",
    "#this by removing redundant values and visualizing the matrix using\n",
    "#seaborn.\n",
    "\n",
    "#Seaborn has been pre-loaded as sns, matplotlib.pyplot as plt,\n",
    "#NumPy as np and pandas as pd.\n",
    "\n",
    "# Create the correlation matrix\n",
    "#corr = ansur_df.corr()\n",
    "\n",
    "# Draw the heatmap\n",
    "#sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.10.svg](_images/13.10.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create a boolean mask for the upper triangle of the plot.\n",
    "\n",
    "# Create the correlation matrix\n",
    "#corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "#mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Add the mask to the heatmap\n",
    "#sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.11.svg](_images/13.11.svg)\n",
    "The buttock and crotch height have a 0.93 correlation coefficient."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Removing highly correlated features**\n",
    "___\n",
    "- correlation caveats - Anscombe's quartet\n",
    "    - nonlinear relationships or datasets with outliers may also correlate strongly\n",
    "    - always visualize the scatterplot\n",
    "- correlation does not imply causation\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Filtering out highly correlated features\n",
    "#You're going to automate the removal of highly correlated features\n",
    "#in the numeric ANSUR dataset. You'll calculate the correlation\n",
    "#matrix and filter out columns that have a correlation coefficient\n",
    "#of more than 0.95 or less than -0.95.\n",
    "\n",
    "#Since each correlation coefficient occurs twice in the matrix\n",
    "#(correlation of A to B equals correlation of B to A) you'll want\n",
    "#to ignore half of the correlation matrix so that only one of the\n",
    "#two correlated features is removed. Use a mask trick for this\n",
    "#purpose.\n",
    "\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "#corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "#mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "#tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "#to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "#reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "#print(\"The reduced_df dataframe has {} columns\".format(reduced_df.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#The original dataframe has 99 columns.\n",
    "#\n",
    "#<script.py> output:\n",
    "#    The reduced_df dataframe has 88 columns\n",
    "#################################################\n",
    "# You've automated the removal of highly correlated features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Nuclear energy and pool drownings\n",
    "\n",
    "#The dataset that has been pre-loaded for you as weird_df contains\n",
    "#actual data provided by the US Centers for Disease Control &\n",
    "#Prevention and Department of Energy.\n",
    "\n",
    "#Let's see if we can find a pattern.\n",
    "\n",
    "#Seaborn has been pre-loaded as sns and matplotlib.pyplot as plt.\n",
    "\n",
    "# Print the first five lines of weird_df\n",
    "#print(weird_df.head())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       pool_drownings  nuclear_energy\n",
    "#    0             421           728.3\n",
    "#    1             465           753.9\n",
    "#    2             494           768.8\n",
    "#    3             538           780.1\n",
    "#    4             430           763.7\n",
    "#################################################\n",
    "\n",
    "#Create a scatterplot with nuclear energy production on the x-axis\n",
    "#and the number of pool drownings on the y-axis.\n",
    "\n",
    "#sns.scatterplot(x='nuclear_energy', y='pool_drownings', data=weird_df)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.12.svg](_images/13.12.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print out the correlation matrix of weird_df\n",
    "#print(weird_df.corr())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                    pool_drownings  nuclear_energy\n",
    "#    pool_drownings        1.000000        0.901179\n",
    "#    nuclear_energy        0.901179        1.000000\n",
    "#################################################\n",
    "#While the example is silly, you'll be amazed how often people\n",
    "#misunderstand correlation vs causation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Selecting features for model performance**\n",
    "___\n",
    "- feature coefficients can be used to drop features that do not contribute to the model\n",
    "- Recursive Feature Elimination (RFE)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Building a diabetes classifier\n",
    "\n",
    "#You'll be using the Pima Indians diabetes dataset to predict whether\n",
    "#a person has diabetes using logistic regression. There are 8 features\n",
    "#and one target in this dataset. The data has been split into a\n",
    "#training and test set and pre-loaded for you as X_train, y_train,\n",
    "#X_test, and y_test.\n",
    "\n",
    "#A StandardScaler() instance has been predefined as scaler and a\n",
    "#LogisticRegression() one as lr.\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "#X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "#lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scale the test features\n",
    "#X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "#y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Prints accuracy metrics and feature coefficients\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred)))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "#################################################\n",
    "#script.py> output:\n",
    "#    79.6% accuracy on test set.\n",
    "#    {'bmi': 0.38, 'glucose': 1.23, 'pregnant': 0.04, 'age': 0.34, 'triceps': 0.24, 'insulin': 0.19, 'family': 0.34, 'diastolic': 0.03}\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Manual Recursive Feature Elimination\n",
    "\n",
    "#Now that we've created a diabetes classifier, let's see if we can\n",
    "#reduce the number of features without hurting the model accuracy too\n",
    "#much.\n",
    "\n",
    "#On the second line of code the features are selected from the\n",
    "#original dataframe. Adjust this selection.\n",
    "\n",
    "#A StandardScaler() instance has been predefined as scaler and a\n",
    "#LogisticRegression() one as lr.\n",
    "\n",
    "#All necessary functions and packages have been pre-loaded too.\n",
    "\n",
    "#First, run the given code, then remove the feature with the lowest\n",
    "#model coefficient from X.\n",
    "\n",
    "# Remove the feature with the lowest model coefficient\n",
    "#X = diabetes_df[['pregnant', 'glucose', 'diastolic', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "#lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "#acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "#################################################\n",
    "#79.6% accuracy on test set.\n",
    "#{'bmi': 0.38, 'glucose': 1.23, 'pregnant': 0.04, 'age': 0.34, 'triceps': 0.24, 'insulin': 0.19, 'family': 0.34, 'diastolic': 0.03}\n",
    "#################################################\n",
    "\n",
    "# Remove the feature with the lowest model coefficient\n",
    "#X = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "#lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "#acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    80.6% accuracy on test set.\n",
    "#    {'bmi': 0.39, 'glucose': 1.23, 'pregnant': 0.05, 'age': 0.35, 'triceps': 0.24, 'insulin': 0.2, 'family': 0.34}\n",
    "#################################################\n",
    "\n",
    "# Remove the 2 features with the lowest model coefficients\n",
    "#X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "#lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "#acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    79.6% accuracy on test set.\n",
    "#    {'triceps': 0.25, 'glucose': 1.13, 'family': 0.34, 'age': 0.37, 'bmi': 0.34}\n",
    "#################################################\n",
    "\n",
    "# Only keep the feature with the highest coefficient\n",
    "#X = diabetes_df[['glucose']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model to the data\n",
    "#lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "#acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    76.5% accuracy on test set.\n",
    "#    {'glucose': 1.27}\n",
    "#################################################\n",
    "#Removing all but one feature only reduced the accuracy by a few percent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Automatic Recursive Feature Elimination\n",
    "\n",
    "#Now let's automate this recursive process. Wrap a Recursive Feature\n",
    "#Eliminator (RFE) around our logistic regression estimator and pass\n",
    "#it the desired number of features.\n",
    "\n",
    "#All the necessary functions and packages have been pre-loaded and\n",
    "#the features have been scaled for you.\n",
    "\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "#rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fit the eliminator to the data\n",
    "#rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "#print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "#print(X.columns[rfe.support_])\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "#acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 8 features.\n",
    "#    Fitting estimator with 7 features.\n",
    "#    Fitting estimator with 6 features.\n",
    "#    Fitting estimator with 5 features.\n",
    "#    Fitting estimator with 4 features.\n",
    "#    {'diastolic': 6, 'bmi': 1, 'glucose': 1, 'pregnant': 5, 'triceps': 3, 'age': 1, 'insulin': 4, 'family': 2}\n",
    "#    Index(['glucose', 'bmi', 'age'], dtype='object')\n",
    "#    80.6% accuracy on test set.\n",
    "#################################################\n",
    "#When we eliminate all but the 3 most relevant features we get a 80.6% accuracy on the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tree-based feature selection**\n",
    "___\n",
    "- feature_importances_ attribute for features in random forest classifiers\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Building a random forest model\n",
    "\n",
    "#You'll again work on the Pima Indians dataset to predict whether\n",
    "#an individual has diabetes. This time using a random forest\n",
    "#classifier. You'll fit the model on the training data after\n",
    "#performing the train-test split and consult the feature importance\n",
    "#values.\n",
    "\n",
    "#The feature and target datasets have been pre-loaded for you as X\n",
    "#and y. Same goes for the necessary packages and functions.\n",
    "\n",
    "# Perform a 75% training and 25% test data split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "#rf = RandomForestClassifier(random_state=0)\n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "#acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "#print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    {'diastolic': 0.08, 'bmi': 0.09, 'glucose': 0.21, 'pregnant': 0.09, 'triceps': 0.11, 'age': 0.16, 'insulin': 0.13, 'family': 0.12}\n",
    "#    77.6% accuracy on test set.\n",
    "#################################################\n",
    "#The random forest model gets 78% accuracy on the test set and 'glucose' is the most important feature (0.21)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Random forest for feature selection\n",
    "#Now lets use the fitted random model to select the most important\n",
    "#features from our input dataset X.\n",
    "\n",
    "#The trained model from the previous exercise has been pre-loaded\n",
    "#for you as rf.\n",
    "\n",
    "#Create a mask for features with an importance higher than 0.15.\n",
    "# Create a mask for features importances above the threshold\n",
    "#mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Prints out the mask\n",
    "#print(mask)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [False  True False False False False False  True]\n",
    "#################################################\n",
    "\n",
    "#Sub-select the most important features by applying the mask to X.\n",
    "# Apply the mask to the feature dataset X\n",
    "#reduced_X = X.loc[:, mask]\n",
    "\n",
    "# prints out the selected column names\n",
    "#print(reduced_X.columns)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['glucose', 'age'], dtype='object')\n",
    "#################################################\n",
    "#Only the features 'glucose' and 'age' were considered sufficiently important."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Recursive Feature Elimination with random forests\n",
    "\n",
    "#You'll wrap a Recursive Feature Eliminator around a random forest\n",
    "#model to remove features step by step. This method is more\n",
    "#conservative compared to selecting features after applying a single\n",
    "#importance threshold. Since dropping one feature can influence the\n",
    "#relative importances of the others.\n",
    "\n",
    "#You'll need these pre-loaded datasets: X, X_train, y_train.\n",
    "\n",
    "#Functions and classes that have been pre-loaded for you are:\n",
    "#RandomForestClassifier(), RFE(), train_test_split().\n",
    "\n",
    "#Create a recursive feature eliminator that will select the 2 most\n",
    "#important features using a random forest model.\n",
    "\n",
    "# Wrap the feature eliminator around the random forest model\n",
    "#rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n",
    "\n",
    "#Fit the recursive feature eliminator to the training data.\n",
    "# Fit the model to the training data\n",
    "#rfe.fit(X_train, y_train)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 8 features.\n",
    "#    Fitting estimator with 7 features.\n",
    "#    Fitting estimator with 6 features.\n",
    "#    Fitting estimator with 5 features.\n",
    "#    Fitting estimator with 4 features.\n",
    "#    Fitting estimator with 3 features.\n",
    "#################################################\n",
    "\n",
    "#Create a mask using the fitted eliminator, then apply it to the\n",
    "#feature dataset X.\n",
    "\n",
    "# Create a mask using an attribute of rfe\n",
    "#mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "#reduced_X = X.loc[:, mask]\n",
    "#print(reduced_X.columns)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['glucose', 'insulin'], dtype='object')\n",
    "#################################################\n",
    "\n",
    "#Change the settings of RFE() to eliminate 2 features at each step.\n",
    "# Set the feature eliminator to remove 2 features on each step\n",
    "#rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "#rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask\n",
    "#mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "#reduced_X = X.loc[:, mask]\n",
    "#print(reduced_X.columns)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 8 features.\n",
    "#    Fitting estimator with 6 features.\n",
    "#    Fitting estimator with 4 features.\n",
    "#    Index(['glucose', 'insulin'], dtype='object')\n",
    "#################################################\n",
    "#Compared to the quick and dirty single threshold method from the\n",
    "#previous exercise one of the selected features is different."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Regularized linear regression**\n",
    "___\n",
    "- .coef_ attribute\n",
    "- R squared value --> .score()\n",
    "- regularization --> simplify MSE values, solve for overfitting\n",
    "    - alpha - too low overfitting, too high innacurate / underfitting\n",
    "    - LASSO - least absolute shrinkage and selection operator\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating a LASSO regressor\n",
    "\n",
    "#You'll be working on the numeric ANSUR body measurements dataset to\n",
    "#predict a persons Body Mass Index (BMI) using the pre-imported\n",
    "#Lasso() regressor. BMI is a metric derived from body height and\n",
    "#weight but those two features have been removed from the dataset\n",
    "#to give the model a challenge.\n",
    "\n",
    "#You'll standardize the data first using the StandardScaler() that\n",
    "#has been instantiated for you as scaler to make sure all\n",
    "#coefficients face a comparable regularizing force trying to bring\n",
    "#them down.\n",
    "\n",
    "#All necessary functions and classes plus the input datasets X and\n",
    "#y have been pre-loaded.\n",
    "\n",
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "#X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model\n",
    "#la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "#la.fit(X_train_std, y_train)\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "#You've fitted the Lasso model to the standardized training data.\n",
    "#Now let's look at the results!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Lasso model results\n",
    "#Now that you've trained the Lasso model, you'll score its predictive\n",
    "#capacity (R2) on the test set and count how many features are\n",
    "#ignored because their coefficient is reduced to zero.\n",
    "\n",
    "#The X_test and y_test datasets have been pre-loaded for you.\n",
    "\n",
    "#The Lasso() model and StandardScaler() have been instantiated as\n",
    "#la and scaler respectively and both were fitted to the training data.\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "#X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "#r_squared = la.score(X_test_std, y_test)\n",
    "#print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "#zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "#n_ignored = sum(zero_coef)\n",
    "#print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The model can predict 84.7% of the variance in the test set.\n",
    "#    The model has ignored 82 out of 91 features.\n",
    "#################################################\n",
    "# We can predict almost 85% of the variance in the BMI value using\n",
    "#just 9 out of 91 of the features. The R^2 could be higher though."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Adjusting the regularization strength\n",
    "#Your current Lasso model has an R2 score of 84.7%. When a model\n",
    "#applies overly powerful regularization it can suffer from high\n",
    "#bias, hurting its predictive power.\n",
    "\n",
    "#Let's improve the balance between predictive power and model\n",
    "#simplicity by tweaking the alpha parameter.\n",
    "\n",
    "#Find the highest value for alpha that gives an R2 value above 98%\n",
    "#from the options: 1, 0.5, 0.1, and 0.01.\n",
    "\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "#la = Lasso(alpha=0.1, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "#la.fit(X_train_std, y_train)\n",
    "#r_squared = la.score(X_test_std, y_test)\n",
    "#n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats\n",
    "#print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "#print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))\n",
    "\n",
    "#################################################\n",
    "#alpha = 1\n",
    "#The model can predict 84.7% of the variance in the test set.\n",
    "#82 out of 91 features were ignored.\n",
    "\n",
    "#alpha = 0.5\n",
    "#The model can predict 93.8% of the variance in the test set.\n",
    "#79 out of 91 features were ignored.\n",
    "\n",
    "#alpha = 0.1\n",
    "#The model can predict 98.3% of the variance in the test set.\n",
    "#64 out of 91 features were ignored.\n",
    "\n",
    "#alpha = 0.01\n",
    "#The model can predict 98.8% of the variance in the test set.\n",
    "#37 out of 91 features were ignored.\n",
    "#################################################\n",
    "#With this more appropriate regularization strength we can predict\n",
    "#98% of the variance in the BMI value while ignoring 2/3 of the features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Combining feature selectors**\n",
    "___\n",
    "- LassoCV() automates the Lasso alpha selection process\n",
    "- Random forest is a combination of decision trees (weak predictors)\n",
    "- we can use combination of models for feature selection too (by rank votes)\n",
    "    - Gradient Boosting\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating a LassoCV regressor\n",
    "\n",
    "#You'll be predicting biceps circumference on a subsample of the\n",
    "#male ANSUR dataset using the LassoCV() regressor that automatically\n",
    "#tunes the regularization strength (alpha value) using Cross-Validation.\n",
    "\n",
    "#The standardized training and test data has been pre-loaded for you\n",
    "#as X_train, X_test, y_train, and y_test.\n",
    "\n",
    "#from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "#lcv = LassoCV()\n",
    "#lcv.fit(X_train, y_train)\n",
    "#print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "#r_squared = lcv.score(X_test, y_test)\n",
    "#print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "#lcv_mask = lcv.coef_ != 0\n",
    "#print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Optimal alpha = 0.089\n",
    "#    The model explains 88.2% of the test set variance\n",
    "#    26 features out of 32 selected\n",
    "#################################################\n",
    "#We got a decent R squared and removed 6 features. We'll save the\n",
    "#lcv_mask for later on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Ensemble models for extra votes\n",
    "#The LassoCV() model selected 26 out of 32 features. Not bad, but\n",
    "#not a spectacular dimensionality reduction either. Let's use two\n",
    "#more models to select the 10 features they consider most important\n",
    "#using the Recursive Feature Eliminator (RFE).\n",
    "\n",
    "#The standardized training and test data has been pre-loaded for\n",
    "#you as X_train, X_test, y_train, and y_test.\n",
    "\n",
    "#from sklearn.feature_selection import RFE\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "#rfe_gb = RFE(estimator=GradientBoostingRegressor(),\n",
    "#             n_features_to_select=10, step=3, verbose=1)\n",
    "#rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "#r_squared = rfe_gb.score(X_test, y_test)\n",
    "#print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "#gb_mask = rfe_gb.support_\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 32 features.\n",
    "#    Fitting estimator with 29 features.\n",
    "#    Fitting estimator with 26 features.\n",
    "#    Fitting estimator with 23 features.\n",
    "#    Fitting estimator with 20 features.\n",
    "#    Fitting estimator with 17 features.\n",
    "#    Fitting estimator with 14 features.\n",
    "#    Fitting estimator with 11 features.\n",
    "#    The model can explain 85.6% of the variance in the test set\n",
    "#################################################\n",
    "\n",
    "#Modify the first step to select 10 features with RFE on a\n",
    "#RandomForestRegressor() and drop 3 features on each step.\n",
    "\n",
    "#from sklearn.feature_selection import RFE\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "#rfe_rf = RFE(estimator=RandomForestRegressor(),\n",
    "#             n_features_to_select=10, step=3, verbose=1)\n",
    "#rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "#r_squared = rfe_rf.score(X_test, y_test)\n",
    "#print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "#rf_mask = rfe_rf.support_\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 32 features.\n",
    "#    Fitting estimator with 29 features.\n",
    "#    Fitting estimator with 26 features.\n",
    "#    Fitting estimator with 23 features.\n",
    "#    Fitting estimator with 20 features.\n",
    "#    Fitting estimator with 17 features.\n",
    "#    Fitting estimator with 14 features.\n",
    "#    Fitting estimator with 11 features.\n",
    "#    The model can explain 84.0% of the variance in the test set\n",
    "#################################################\n",
    "#Inluding the Lasso linear model from the previous exercise, we\n",
    "#now have the votes from 3 models on which features are important."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#Combining 3 feature selectors\n",
    "#We'll combine the votes of the 3 models you built in the previous\n",
    "#exercises, to decide which features are important into a meta mask.\n",
    "#We'll then use this mask to reduce dimensionality and see how a\n",
    "#simple linear regressor performs on the reduced dataset.\n",
    "\n",
    "#The per model votes have been pre-loaded as lcv_mask, rf_mask, and\n",
    "#gb_mask and the feature and target datasets as X and y.\n",
    "\n",
    "# Sum the votes of the three models\n",
    "#votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "#print(votes)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [1 0 2 2 0 1 0 3 1 1 1 3 1 1 1 3 0 1 1 2 1 1 2 1 1 3 2 1 3 1 3 3]\n",
    "#################################################\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "#meta_mask = votes >= 3\n",
    "#print(meta_mask)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [False False False False False False False  True False False False  True\n",
    "#     False False False  True False False False False False False False False\n",
    "#     False  True False False  True False  True  True]\n",
    "#################################################\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "#X_reduced = X.loc[:,meta_mask]\n",
    "#print(X_reduced.columns)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['chestcircumference', 'forearmcircumferenceflexed', 'hipbreadth', 'thighcircumference', 'waistcircumference', 'wristheight', 'BMI'], dtype='object')\n",
    "#################################################\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "#lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "#r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "#print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The model can explain 86.8% of the variance in the test set using 7 features.\n",
    "#################################################\n",
    "#Using the votes from 3 models you were able to select just 7 features\n",
    "#that allowed a simple linear model to get a high accuracy!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature Extraction**\n",
    "___\n",
    "- calculate new features using existing ones while losing as little information as possible\n",
    "    - e.g., taking averages of multiple features\n",
    "- Principal Component Analysis (PCA)\n",
    "    - be sure to scale features\n",
    "    - multiply and sum components on perpendicular axes\n",
    "    - accounts for all variance in the data\n",
    "    - creates a new reference\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Manual feature extraction I\n",
    "\n",
    "#You want to compare prices for specific products between stores.\n",
    "#The features in the pre-loaded dataset sales_df are: storeID,\n",
    "#product, quantity and revenue. The quantity and revenue features\n",
    "#tell you how many items of a particular product were sold in a\n",
    "#store and what the total revenue was. For the purpose of your\n",
    "#analysis it's more interesting to know the average price per\n",
    "#product.\n",
    "\n",
    "#################################################\n",
    "#  storeID  product  quantity  revenue\n",
    "#0       A   Apples      1811   9300.6\n",
    "#1       A  Bananas      1003   3375.2\n",
    "#2       A  Oranges      1604   8528.5\n",
    "#3       B   Apples      1785   9181.0\n",
    "#4       B  Bananas       944   3680.2\n",
    "#################################################\n",
    "# Calculate the price from the quantity sold and revenue\n",
    "#sales_df['price'] = sales_df['revenue'] / sales_df['quantity']\n",
    "\n",
    "# Drop the quantity and revenue features\n",
    "#reduced_df = sales_df.drop(['revenue', 'quantity'], axis=1)\n",
    "\n",
    "#print(reduced_df.head())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#      storeID  product     price\n",
    "#    0       A   Apples  5.135616\n",
    "#    1       A  Bananas  3.365105\n",
    "#    2       A  Oranges  5.317020\n",
    "#    3       B   Apples  5.143417\n",
    "#    4       B  Bananas  3.898517\n",
    "#################################################\n",
    "#When you understand the dataset well, always check if you can\n",
    "#calculate relevant features and drop irrelevant ones."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Manual feature extraction II\n",
    "#You're working on a variant of the ANSUR dataset, height_df,\n",
    "#where a person's height was measured 3 times. Add a feature with\n",
    "#the mean height to the dataset, then drop the 3 original features.\n",
    "\n",
    "#################################################\n",
    "#  weight_kg  height_1  height_2  height_3\n",
    "#0       81.5      1.78      1.80      1.80\n",
    "#1       72.6      1.70      1.70      1.69\n",
    "#2       92.9      1.74      1.75      1.73\n",
    "#3       79.4      1.66      1.68      1.67\n",
    "#4       94.6      1.91      1.93      1.90\n",
    "#################################################\n",
    "# Calculate the mean height\n",
    "#height_df['height'] = height_df[['height_1', 'height_2', 'height_3']].mean(axis=1)\n",
    "\n",
    "# Drop the 3 original height features\n",
    "#reduced_df = height_df.drop(['height_1', 'height_2', 'height_3'], axis=1)\n",
    "\n",
    "#print(reduced_df.head())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       weight_kg    height\n",
    "#    0       81.5  1.793333\n",
    "#    1       72.6  1.696667\n",
    "#    2       92.9  1.740000\n",
    "#    3       79.4  1.670000\n",
    "#    4       94.6  1.913333\n",
    "#################################################\n",
    "#You've calculated a new feature that is still easy to understand\n",
    "#compared to, for instance, principal components."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Principal component analysis**\n",
    "___\n",
    "- PCA removes correlation\n",
    "- amount of features - amount of principal components\n",
    "- explained variance ratio - .explained_variance_ratio_\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Calculating Principal Components\n",
    "\n",
    "#You'll visually inspect a 4 feature sample of the ANSUR dataset\n",
    "#before and after PCA using Seaborn's pairplot(). This will allow\n",
    "#you to inspect the pairwise correlations between the features.\n",
    "\n",
    "#The data has been pre-loaded for you as ansur_df.\n",
    "\n",
    "# Create a pairplot to inspect ansur_df\n",
    "#sns.pairplot(ansur_df)\n",
    "\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.13.svg](_images/13.13.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler\n",
    "#scaler = StandardScaler()\n",
    "#ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "#pca = PCA()\n",
    "#pc = pca.fit_transform(ansur_std)\n",
    "#pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "#sns.pairplot(pc_df)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.14.svg](_images/13.14.svg)\n",
    "Notice how, in contrast to the input features, none of the principal\n",
    "components are correlated to one another."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#PCA on a larger dataset\n",
    "\n",
    "#You'll now apply PCA on a somewhat larger ANSUR datasample with\n",
    "#13 dimensions, once again pre-loaded as ansur_df. The fitted model\n",
    "#will be used in the next exercise. Since we are not using the\n",
    "#principal components themselves there is no need to transform the\n",
    "#data, instead, it is sufficient to fit pca to the data.\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the data\n",
    "#scaler = StandardScaler()\n",
    "#ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Apply PCA\n",
    "#pca = PCA()\n",
    "#pca.fit(ansur_std)\n",
    "\n",
    "#You've fitted PCA on our 13 feature datasample. Now let's see how\n",
    "#the components explain the variance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#PCA explained variance\n",
    "\n",
    "#You'll be inspecting the variance explained by the different\n",
    "#principal components of the pca instance you created in the\n",
    "#previous exercise.\n",
    "\n",
    "# Inspect the explained variance ratio per component\n",
    "#print(pca.explained_variance_ratio_)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0.61449404 0.19893965 0.06803095 0.03770499 0.03031502 0.0171759\n",
    "#     0.01072762 0.00656681 0.00634743 0.00436015 0.0026586  0.00202617\n",
    "#     0.00065268]\n",
    "#################################################\n",
    "\n",
    "# Print the cumulative sum of the explained variance ratio\n",
    "#print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0.61449404 0.81343368 0.88146463 0.91916962 0.94948464 0.96666054\n",
    "#     0.97738816 0.98395496 0.99030239 0.99466254 0.99732115 0.99934732\n",
    "#     1.        ]\n",
    "#################################################\n",
    "#Using just 4 principal components we can explain more than 90%\n",
    "#of the variance in the 13 feature dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**PCA applications**\n",
    "___\n",
    "- remaining components can be hard to interpret\n",
    "- look at components attribute pca.components_\n",
    "    - to what extent does a components vector is affected by a specific feature\n",
    "- checking effect of categorical features\n",
    "    - hue category for scatterplot\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Understanding the components\n",
    "\n",
    "#You'll apply PCA to the numeric features of the Pokemon dataset,\n",
    "#poke_df, using a pipeline to combine the feature scaling and PCA\n",
    "#in one go. You'll then interpret the meanings of the first two\n",
    "#components.\n",
    "\n",
    "#All relevant packages and classes have been pre-loaded for you\n",
    "#(Pipeline(), StandardScaler(), PCA()).\n",
    "\n",
    "# Build the pipeline\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#        \t\t ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "#pipe.fit(poke_df)\n",
    "#vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "#print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "#print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    PC 1 effects = {'Defense': 0.36, 'Attack': 0.44, 'Sp. Def': 0.45, 'Sp. Atk': 0.46, 'HP': 0.39, 'Speed': 0.34}\n",
    "#    PC 2 effects = {'Defense': 0.63, 'Attack': -0.01, 'Sp. Def': 0.24, 'Sp. Atk': -0.31, 'HP': 0.08, 'Speed': -0.67}\n",
    "#################################################\n",
    "#PC1: All features have a similar positive effect. PC 1 can be\n",
    "#interpreted as a measure of overall quality (high stats)\n",
    "#PC2: Defense has a strong positive effect on the second component\n",
    "#and speed a strong negative one. This component quantifies an\n",
    "#agility vs. armor & protection trade-off."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#PCA for feature exploration\n",
    "\n",
    "#You'll use the PCA pipeline you've built in the previous exercise\n",
    "#to visually explore how some categorical features relate to the\n",
    "#variance in poke_df. These categorical features (Type & Legendary)\n",
    "#can be found in a separate dataframe poke_cat_df.\n",
    "\n",
    "#All relevant packages and classes have been pre-loaded for you\n",
    "#(Pipeline(), StandardScaler(), PCA())\n",
    "\n",
    "# Build the pipeline\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "#pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "#print(pc)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [[-1.5563747  -0.02148212]\n",
    "#     [-0.36286656 -0.05026854]\n",
    "#     [ 1.28015158 -0.06272022]\n",
    "#     ...\n",
    "#     [ 2.45821626 -0.51588158]\n",
    "#     [ 3.5303971  -0.95106516]\n",
    "#     [ 2.23378629  0.53762985]]\n",
    "#################################################\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "#poke_cat_df['PC 1'] = pc[:, 0]\n",
    "#poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "#pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "#poke_cat_df['PC 1'] = pc[:, 0]\n",
    "#poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
    "#sns.scatterplot(data=poke_cat_df,\n",
    "#                x='PC 1', y='PC 2', hue='Type')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.15.svg](_images/13.15.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#################################################\n",
    "#<script.py> output:\n",
    "#        Type  Legendary      PC 1      PC 2\n",
    "#    0  Grass      False -1.556375 -0.021482\n",
    "#    1  Grass      False -0.362867 -0.050269\n",
    "#    2  Grass      False  1.280152 -0.062720\n",
    "#    3  Grass      False  2.620916  0.704263\n",
    "#################################################\n",
    "# Use the Legendary feature to color the PC 1 vs PC 2 scatterplot\n",
    "#sns.scatterplot(data=poke_cat_df,\n",
    "#                x='PC 1', y='PC 2', hue='Legendary')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.16.svg](_images/13.16.svg)\n",
    "Looks like the different types are scattered all over the place\n",
    "while the legendary Pokemon always score high for PC 1 meaning they\n",
    "have high stats overall. Their spread along the PC 2 axis tells us\n",
    "they aren't consistently fast and vulnerable or slow and armored."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#PCA in a model pipeline\n",
    "\n",
    "#We just saw that legendary Pokemon tend to have higher stats\n",
    "#overall. Let's see if we can add a classifier to our pipeline that\n",
    "#detects legendary versus non-legendary Pokemon based on the\n",
    "#principal components.\n",
    "\n",
    "#The data has been pre-loaded for you and split into training and\n",
    "#tests datasets: X_train, X_test, y_train, y_test.\n",
    "\n",
    "#Same goes for all relevant packages and classes(Pipeline(),\n",
    "#StandardScaler(), PCA(), RandomForestClassifier()).\n",
    "\n",
    "# Build the pipeline\n",
    "#pipe = Pipeline([\n",
    "#        ('scaler', StandardScaler()),\n",
    "#        ('reducer', PCA(n_components=2)),\n",
    "#        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "#pipe.fit(X_train, y_train)\n",
    "\n",
    "# Prints the explained variance ratio\n",
    "#print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0.45624044 0.17767414]\n",
    "#################################################\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "#accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "#print('{0:.1%} test set accuracy'.format(accuracy))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    95.8% test set accuracy\n",
    "#################################################\n",
    "\n",
    "#Repeat the process with 3 extracted components\n",
    "# Build the pipeline\n",
    "#pipe = Pipeline([\n",
    "#        ('scaler', StandardScaler()),\n",
    "#        ('reducer', PCA(n_components=3)),\n",
    "#        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "#pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "#accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the explained variance ratio and accuracy\n",
    "#print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "#print('{0:.1%} test set accuracy'.format(accuracy))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0.45624044 0.17767414 0.12858833]\n",
    "#    95.0% test set accuracy\n",
    "#################################################\n",
    "# Looks like adding the third component does not increase the\n",
    "#model accuracy, even though it adds information to the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Principal Component selection**\n",
    "___\n",
    "- we can set an explained variance threshold instead of number of components\n",
    "    - n_components = 0-1\n",
    "- an optimal number of components\n",
    "    - elbow plot\n",
    "- .inverse_transform goes back from PCA to original components\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Selecting the proportion of variance to keep\n",
    "\n",
    "#You'll let PCA determine the number of components to calculate\n",
    "#based on an explained variance threshold that you decide.\n",
    "\n",
    "#You'll work on the numeric ANSUR female dataset pre-loaded as\n",
    "#ansur_df.\n",
    "\n",
    "#All relevant packages and classes have been pre-loaded too\n",
    "#(Pipeline(), StandardScaler(), PCA()).\n",
    "\n",
    "# Pipe a scaler to PCA selecting 80% of the variance\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#        \t\t ('reducer', PCA(n_components=0.8))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "#pipe.fit(ansur_df)\n",
    "\n",
    "#print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    11 components selected\n",
    "#################################################\n",
    "\n",
    "#Increase the proportion of variance to keep to 90%\n",
    "\n",
    "# Let PCA select 90% of the variance\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#        \t\t ('reducer', PCA(n_components=0.9))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "#pipe.fit(ansur_df)\n",
    "\n",
    "#print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    23 components selected\n",
    "#################################################\n",
    "#We need to more than double the components to go from 80% to 90%\n",
    "#explained variance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Choosing the number of components\n",
    "\n",
    "#You'll now make a more informed decision on the number of principal\n",
    "#components to reduce your data to using the \"elbow in the plot\"\n",
    "#technique. One last time, you'll work on the numeric ANSUR female\n",
    "#dataset pre-loaded as ansur_df.\n",
    "\n",
    "#All relevant packages and classes have been pre-loaded for you\n",
    "#(Pipeline(), StandardScaler(), PCA()).\n",
    "\n",
    "# Pipeline a scaler and pca selecting 10 components\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#        \t\t ('reducer', PCA(n_components=10))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "#pipe.fit(ansur_df)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "#plt.plot(pipe.steps[1][1].explained_variance_ratio_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.17.svg](_images/13.17.svg)\n",
    "The 'elbow' in the plot is at 3 components\n",
    "(the 3rd component has index 2)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#PCA for image compression\n",
    "\n",
    "#You'll reduce the size of 16 images with hand written digits\n",
    "#(MNIST dataset) using PCA.\n",
    "\n",
    "#The samples are 28 by 28 pixel gray scale images that have been\n",
    "#flattened to arrays with 784 elements each (28 x 28 = 784) and\n",
    "#added to the 2D numpy array X_test. Each of the 784 pixels has a\n",
    "#value between 0 and 255 and can be regarded as a feature.\n",
    "\n",
    "#A pipeline with a scaler and PCA model to select 78 components\n",
    "#has been pre-loaded for you as pipe. This pipeline has already\n",
    "#been fitted to the entire MNIST dataset except for the 16 samples\n",
    "#in X_test.\n",
    "\n",
    "#Finally, a function plot_digits has been created for you that\n",
    "#will plot 16 images in a grid.\n",
    "\n",
    "# Plot the MNIST sample data\n",
    "#plot_digits(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.18.svg](_images/13.18.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform the input data to principal components\n",
    "#pc = pipe.transform(X_test)\n",
    "\n",
    "# Prints the number of features per dataset\n",
    "#print(\"X_test has {} features\".format(X_test.shape[1]))\n",
    "#print(\"pc has {} features\".format(pc.shape[1]))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    X_test has 784 features\n",
    "#    pc has 78 features\n",
    "#################################################\n",
    "\n",
    "#Inverse transform the components back to the original feature space.\n",
    "# Inverse transform the components to original feature space\n",
    "#X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Prints the number of features\n",
    "#print(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    X_rebuilt has 784 features\n",
    "#################################################\n",
    "\n",
    "# Plot the reconstructed data\n",
    "#plot_digits(X_rebuilt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/13.19.svg](_images/13.19.svg)\n",
    "You've reduced the size of the data 10 fold but were able to\n",
    "reconstruct images with reasonable quality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Congratulations**\n",
    "___\n",
    "- What you've learned\n",
    "    - Why dimensionality reduction is important & when to use it\n",
    "    - Feature selection vs. extraction\n",
    "    - High dimensional data exploration with t-SNE & PCA\n",
    "    - Use models to find important features\n",
    "    - Remove unimportant features\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}