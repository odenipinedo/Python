{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Classification with XGBoost**\n",
    "___\n",
    "Introduction:\n",
    "- Supervised learning\n",
    "    - **has labeled data** - we have some understanding of past behavior of problem we are trying to solve or what we are trying to predict\n",
    "    - **Classification** - binary or multi-class\n",
    "        - AUC is metric for binary classification\n",
    "            - Area on receiver operating characteristic curve\n",
    "            - probability that a randomly chosen positive data point will have a higher rank than a randomly chosen negative data point for your data problem\n",
    "            - higher AUC, better model\n",
    "        - for multi-class problems, confusion-matrix and accuracy score is how this happens\n",
    "- features are either numeric or categorical\n",
    "    - numeric features should be scaled (e.g., SVM models)\n",
    "    - categorical features should be encoded (one-hot)\n",
    "- **Ranking** - predicting an ordering on a srt of choices\n",
    "- **Recommending** - recommending an item to a user based on consumption history and profile\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction to XGBoost**\n",
    "___\n",
    "- optimized gradient boosting machine learning library\n",
    "- written in C++\n",
    "- fast\n",
    "- paralellizable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#XGBoost: Fit/Predict\n",
    "\n",
    "#It's time to create your first XGBoost model! As Sergey showed you\n",
    "#in the video, you can use the scikit-learn .fit() / .predict()\n",
    "#paradigm that you are already familiar to build your XGBoost models,\n",
    "#as the xgboost library has a scikit-learn compatible API!\n",
    "\n",
    "#Here, you'll be working with churn data. This dataset contains imaginary\n",
    "#data from a ride-sharing app with user behaviors over their first month\n",
    "#of app usage in a set of imaginary cities as well as whether they used\n",
    "#the service 5 months after sign-up. It has been pre-loaded for you into\n",
    "#a DataFrame called churn_data - explore it in the Shell!\n",
    "\n",
    "#Your goal is to use the first month's worth of data to predict whether\n",
    "#the app's users will remain users of the service at the 5 month mark.\n",
    "#This is a typical setup for a churn prediction problem. To do this,\n",
    "#you'll split the data into training and test sets, fit a small xgboost\n",
    "#model on the training set, and evaluate its performance on the test set\n",
    "#by computing its accuracy.\n",
    "\n",
    "#pandas and numpy have been imported as pd and np, and train_test_split\n",
    "#has been imported from sklearn.model_selection. Additionally, the arrays\n",
    "#for the features and the target have been created as X and y.\n",
    "\n",
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "#X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "#X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "#xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "#xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "#preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "#accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "#print(\"accuracy: %f\" % (accuracy))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    accuracy: 0.743300\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What is a decision tree?**\n",
    "___\n",
    "- series of binary choices\n",
    "- prediction happens at the \"leaves\" of the tree\n",
    "- **base learner** - individual learning algorithm in an ensemble algorithm\n",
    "- decision trees and CART are constructed iteratively until a stopping criterion is met\n",
    "- individual decision trees tend to overfit\n",
    "    - low bias, high variance\n",
    "    - generalize to new data poorly\n",
    "- **Classification and Regression Trees (CART)**\n",
    "    - each leaf *always* contains a real-valued score which can be later converted into categories\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Decision trees\n",
    "\n",
    "#Your task in this exercise is to make a simple decision tree using\n",
    "#scikit-learn's DecisionTreeClassifier on the breast cancer dataset\n",
    "#that comes pre-loaded with scikit-learn.\n",
    "\n",
    "#This dataset contains numeric measurements of various dimensions of\n",
    "#individual tumors (such as perimeter and texture) from breast biopsies\n",
    "#and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "#We've preloaded the dataset of samples (measurements) into X and the\n",
    "#target values per tumor into y. Now, you have to split the complete\n",
    "#dataset into training and testing sets, and then train a\n",
    "#DecisionTreeClassifier. You'll specify a parameter called max_depth.\n",
    "#Many other parameters can be modified within this model, and you can\n",
    "#check all of them out at\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "#dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "#dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "#y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "#accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "#print(\"accuracy:\", accuracy)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#   accuracy: 0.9649122807017544\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What is Boosting?**\n",
    "___\n",
    "- a concept that can be applied to a set of machine learning models\n",
    "- ensemble meta-algorithm used to convert many weak learners into a strong learner\n",
    "- **weak learner**\n",
    "    - ML algorithm that is slightly better than chance (50/50)\n",
    "- **strong learner**\n",
    "    - any algorithm that can be tuned to achieve good performance\n",
    "- iteratively learning a set of weak models on subsets of the data\n",
    "- weighting each weak prediction according to each weak learner's performance\n",
    "- combine weighted predictions to obtain a single weighted prediction\n",
    "- **Cross-validation** is baked into XGBoost\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Measuring accuracy\n",
    "\n",
    "#You'll now practice using XGBoost's learning API through its baked\n",
    "#in cross-validation capabilities. As Sergey discussed in the previous\n",
    "#video, XGBoost gets its lauded performance and efficiency gains by\n",
    "#utilizing its own optimized data structure for datasets called a\n",
    "#DMatrix.\n",
    "\n",
    "#In the previous exercise, the input datasets were converted into\n",
    "#DMatrix data on the fly, but when you use the xgboost cv object,\n",
    "#you have to first explicitly convert your data into a DMatrix. So,\n",
    "#that's what you will do here before running cross-validation on\n",
    "#churn_data.\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "#X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "#churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "#cv_results = xgb.cv(dtrain=churn_dmatrix, params=params,\n",
    "#                    nfold=3, num_boost_round=5,\n",
    "#                    metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "#print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       train-error-mean  train-error-std  test-error-mean  test-error-std\n",
    "#    0           0.28232         0.002366          0.28378        0.001932\n",
    "#    1           0.26951         0.001855          0.27190        0.001932\n",
    "#    2           0.25605         0.003213          0.25798        0.003963\n",
    "#    3           0.25090         0.001845          0.25434        0.003827\n",
    "#    4           0.24654         0.001981          0.24852        0.000934\n",
    "#    0.75148\n",
    "#################################################\n",
    "\n",
    "#Measuring AUC\n",
    "#Now that you've used cross-validation to compute average out-of-sample\n",
    "#accuracy (after converting from an error), it's very easy to compute\n",
    "#any other metric you might be interested in. All you have to do is pass\n",
    "#it (or a list of metrics) in as an argument to the metrics parameter\n",
    "#of xgb.cv().\n",
    "\n",
    "#Your job in this exercise is to compute another common metric used in\n",
    "#binary classification - the area under the curve (\"auc\"). As before,\n",
    "#churn_data is available in your workspace, along with the DMatrix\n",
    "#churn_dmatrix and parameter dictionary params.\n",
    "\n",
    "#Perform cross_validation: cv_results\n",
    "#cv_results = xgb.cv(dtrain=churn_dmatrix, params=params,\n",
    "#                  nfold=3, num_boost_round=5,\n",
    "#                  metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "#print((cv_results[\"test-auc-mean\"]).iloc[-1])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
    "#    0        0.768893       0.001544       0.767863      0.002820\n",
    "#    1        0.790864       0.006758       0.789157      0.006846\n",
    "#    2        0.815872       0.003900       0.814476      0.005997\n",
    "#    3        0.822959       0.002018       0.821682      0.003912\n",
    "#    4        0.827528       0.000769       0.826191      0.001937\n",
    "#    0.826191\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**When should I use XGBoost?**\n",
    "___\n",
    "- any supervised learning example with:\n",
    "    - large number of training samples (1000+ samples with fewer than 100 features)\n",
    "    - number of features < number of training samples\n",
    "    - a mixture of categorical and numeric features\n",
    "    - just numeric features\n",
    "- do not use XGBoost with:\n",
    "    - image recognition\n",
    "    - computer vision\n",
    "    - natural language processing/understanding\n",
    "    - smaller number of training samples (see above)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Regression Review**\n",
    "___\n",
    "- Common regression metrics\n",
    "    - root mean squared error (RMSE)\n",
    "        - square root of mean of [difference between actual and predicted values, squared]\n",
    "        - treats negative and positive values equally\n",
    "        - tends to punish larger differences between predicted and actual values\n",
    "    - mean absolute error (MAE)\n",
    "        - sums absolute differences\n",
    "- **Algorithms**\n",
    "    - linear regression\n",
    "    - decision trees\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Objective (loss) functions and base learners**\n",
    "___\n",
    "- Quantifies how far off a prediction is from the actual result\n",
    "- Measures the difference between the estimated true values for some collection of data\n",
    "- **Goal**: find the model that yields the minimum value of the loss function\n",
    "- in xgboost:\n",
    "    - reg:linear - use for regression problems\n",
    "    - reg:logistic - use for classification problems when you want decision, not probability\n",
    "    - binary:logistic - use when you want probability rather than just decision\n",
    "- base learners and why we need them\n",
    "        - we want base learners that when combined create a final prediction that is **non-linear**\n",
    "        - each base learner should be good at distinguishing or predicting different parts of the dataset\n",
    "- two kinds of base learners: tree and linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Decision trees as base learners\n",
    "\n",
    "#It's now time to build an XGBoost model to predict house prices -\n",
    "#not in Boston, Massachusetts, as you saw in the video, but in Ames,\n",
    "#Iowa! This dataset of housing prices has been pre-loaded into a\n",
    "#DataFrame called df. If you explore it in the Shell, you'll see that\n",
    "#there are a variety of features about the house and its location in\n",
    "#the city.\n",
    "\n",
    "#In this exercise, your goal is to use trees as base learners. By default,\n",
    "#XGBoost uses trees as base learners, so you don't have to specify that you\n",
    "#want to use trees here with booster=\"gbtree\".\n",
    "\n",
    "#xgboost has been imported as xgb and the arrays for the features and\n",
    "#the target are available in X and y, respectively.\n",
    "\n",
    "# Create the training and test sets\n",
    "#X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "#xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "#xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "#preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "#rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "#print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [18:40:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
    "#    RMSE: 78847.401758\n",
    "#################################################\n",
    "\n",
    "#Linear base learners\n",
    "\n",
    "#Now that you've used trees as base models in XGBoost, let's use the\n",
    "#other kind of base model that can be used with XGBoost - a linear\n",
    "#learner. This model, although not as commonly used in XGBoost, allows\n",
    "#you to create a regularized linear regression using XGBoost's powerful\n",
    "#learning API. However, because it's uncommon, you have to use XGBoost's\n",
    "#own non-scikit-learn compatible functions to build the model, such as\n",
    "#xgb.train().\n",
    "\n",
    "#In order to do this you must create the parameter dictionary that\n",
    "#describes the kind of booster you want to use (similarly to how you\n",
    "#created the dictionary in Chapter 1 when you used xgb.cv()). The\n",
    "#key-value pair that defines the booster type (base model) you need is\n",
    "#\"booster\":\"gblinear\".\n",
    "\n",
    "#Once you've created the model, you can use the .train() and .predict()\n",
    "#methods of the model just like you've done in the past.\n",
    "\n",
    "#Here, the data has already been split into training and testing sets,\n",
    "#so you can dive right into creating the DMatrix objects required by the\n",
    "#XGBoost learning API.\n",
    "\n",
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "#DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "#DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "#xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "#preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "#rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "#print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [18:52:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
    "#    RMSE: 40738.238504\n",
    "#################################################\n",
    "\n",
    "#Evaluating model quality\n",
    "#It's now time to begin evaluating model quality.\n",
    "\n",
    "#Here, you will compare the RMSE and MAE of a cross-validated XGBoost\n",
    "#model on the Ames housing data. As in previous exercises, all necessary\n",
    "#modules have been pre-loaded and the data is available in the DataFrame\n",
    "#df.\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "#cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#print(cv_results)\n",
    "\n",
    "# Extract and print final round boosting round metric\n",
    "#print((cv_results[\"test-mae-mean\"]).tail(1))\n",
    "\n",
    "#results printed below for \"rmse\" first and \"mae\" second\n",
    "#################################################\n",
    "#train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
    "#    0    141767.531250      429.454591   142980.429688    1193.794436\n",
    "#    1    102832.544922      322.474657   104891.394532    1223.158855\n",
    "#    2     75872.619140      266.472468    79478.935547    1601.344218\n",
    "#    3     57245.650390      273.624608    62411.922851    2220.149653\n",
    "#    4     44401.297851      316.422372    51348.279297    2963.377719\n",
    "#    4    51348.279297\n",
    "#   Name: test-rmse-mean, dtype: float64\n",
    "\n",
    "#train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
    "#    0   127343.476562     668.342129  127633.980469   2404.003469\n",
    "#    1    89770.052735     456.962096   90122.501953   2107.915156\n",
    "#    2    63580.791992     263.403452   64278.561524   1887.563452\n",
    "#    3    45633.153321     151.884551   46819.167969   1459.819091\n",
    "#    4    33587.092774      86.999100   35670.644531   1140.607997\n",
    "#    4    35670.644531\n",
    "#    Name: test-mae-mean, dtype: float64\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Regularization and base learners in XGBoost**\n",
    "___\n",
    "- Regularization is a control on model complexity\n",
    "- Want models that are both accurate and as simple as possible\n",
    "- Regularization parameters in XGBoost\n",
    "    - *gamma* - minimum loss reduction allowed for a split to occur (higher value means fewer splits)\n",
    "    - *alpha* - l1 regularization (many weights will go to zero) of leaf weights (higher values mean more regularization)\n",
    "    - *lambda* - l2 regularization (smooths leaf weights)\n",
    "- Linear base learner\n",
    "    - sum of linear terms\n",
    "    - boosted model is weighted sum of linear models (linear)\n",
    "    - rarely used (you can get same performance from regularized linear model\n",
    "- Tree base learner\n",
    "    - decision tree\n",
    "    - boosted model is weighted sum of decision trees (nonlinear)\n",
    "    - almost always used in XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using regularization in XGBoost\n",
    "#Having seen an example of l1 regularization in the video, you'll now\n",
    "#vary the l2 regularization penalty - also known as \"lambda\" - and see\n",
    "#its effect on overall model performance on the Ames housing dataset.\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "#reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "#params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "#rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "#for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "#    params[\"lambda\"] = reg\n",
    "\n",
    "    # Pass this updated param dictionary into cv\n",
    "#    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "#    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "#print(\"Best rmse as a function of l2:\")\n",
    "#print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\",\"rmse\"]))\n",
    "\n",
    "#################################################\n",
    "#Best rmse as a function of l2:\n",
    "#        l2          rmse\n",
    "#    0    1  52275.359375\n",
    "#    1   10  57746.064453\n",
    "#    2  100  76624.625000\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing individual XGBoost trees\n",
    "#Now that you've used XGBoost to both build and evaluate regression as\n",
    "#well as classification models, you should get a handle on how to\n",
    "#visually explore your models. Here, you will visualize individual trees\n",
    "#from the fully boosted model that XGBoost creates using the entire\n",
    "#housing dataset.\n",
    "\n",
    "#XGBoost has a plot_tree() function that makes this type of\n",
    "#visualization easy. Once you train a model using the XGBoost learning\n",
    "#API, you can pass it to the plot_tree() function along with the number\n",
    "#of trees you want to plot using the num_trees argument.\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "#xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "#xgb.plot_tree(xg_reg, num_trees=0)\n",
    "#plt.show()\n",
    "\n",
    "# Plot the fifth tree\n",
    "#xgb.plot_tree(xg_reg, num_trees=4)\n",
    "#plt.show()\n",
    "\n",
    "# Plot the last tree sideways\n",
    "#xgb.plot_tree(xg_reg, num_trees=9, rankdir=\"LR\")\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/11.1.svg](_images/11.1.svg)\n",
    "![_images/11.2.svg](_images/11.2.svg)\n",
    "![_images/11.3.svg](_images/11.3.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing feature importances: What features are most important in\n",
    "#my dataset\n",
    "\n",
    "#Another way to visualize your XGBoost models is to examine the\n",
    "#importance of each feature column in the original dataset within the\n",
    "#model.\n",
    "\n",
    "#One simple way of doing this involves counting the number of times\n",
    "#each feature is split on across all boosting rounds (trees) in the\n",
    "#model, and then visualizing the result as a bar graph, with the\n",
    "#features ordered according to how many times they appear. XGBoost has\n",
    "#a plot_importance() function that allows you to do exactly this, and\n",
    "#you'll get a chance to use it in this exercise!\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "#xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "#xgb.plot_importance(xg_reg)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/11.4.svg](_images/11.4.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Why tune your model?**\n",
    "___\n",
    "- ~10-15% reduction of RMSE\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Tuning the number of boosting rounds\n",
    "#Let's start with parameter tuning by seeing how the number of boosting\n",
    "#rounds (number of trees you build) impacts the out-of-sample performance\n",
    "#of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\n",
    "\n",
    "#Here, you'll continue working with the Ames housing dataset. The features\n",
    "#are available in the array X, and the target vector is contained in y.\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(X,y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "#params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "#num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "#final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "#for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "#    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "    # Append final round RMSE\n",
    "#    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "#num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "#print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))\n",
    "\n",
    "#################################################\n",
    "#   num_boosting_rounds          rmse\n",
    "#                   5       50903.299479\n",
    "#                   10      34774.192708\n",
    "#                   15      32895.098958\n",
    "#################################################\n",
    "\n",
    "#Automated boosting round selection using early_stopping\n",
    "\n",
    "#Now, instead of attempting to cherry pick the best possible number\n",
    "#of boosting rounds, you can very easily have XGBoost automatically\n",
    "#select the number of boosting rounds for you within xgb.cv(). This is\n",
    "#done using a technique called early stopping.\n",
    "\n",
    "#Early stopping works by testing the XGBoost model after every boosting\n",
    "#round against a hold-out dataset and stopping the creation of additional\n",
    "#boosting rounds (thereby finishing training of the model early) if the\n",
    "#hold-out metric (\"rmse\" in our case) does not improve for a given number\n",
    "#of rounds. Here you will use the early_stopping_rounds parameter in\n",
    "#xgb.cv() with a large possible number of boosting rounds (50). Bear in\n",
    "#mind that if the holdout metric continuously improves up through when\n",
    "#num_boost_rounds is reached, then early stopping does not occur.\n",
    "\n",
    "#Here, the DMatrix and parameter dictionary have been created for you.\n",
    "#Your task is to use cross-validation with early stopping. Go for it!\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "#params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "#cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "#                    params=params,\n",
    "#                    nfold=3,\n",
    "#                    early_stopping_rounds=10,\n",
    "#                    num_boost_round=50,\n",
    "#                    metrics=\"rmse\",\n",
    "#                    as_pandas=True,\n",
    "#                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#print(cv_results)\n",
    "\n",
    "#################################################\n",
    "#train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
    "#    0     141871.635417      403.636200   142640.651042     705.559164\n",
    "#    1     103057.033854       73.772531   104907.664062     111.112417\n",
    "#    2      75975.966146      253.726099    79262.059895     563.766991\n",
    "#    3      57420.531250      521.656754    61620.136719    1087.694282\n",
    "#    4      44552.955729      544.170190    50437.561198    1846.446330\n",
    "#    5      35763.946615      681.797429    43035.661458    2034.469207\n",
    "#    6      29861.464193      769.571238    38600.880208    2169.796232\n",
    "#    7      25994.676432      756.520565    36071.817708    2109.795430\n",
    "#    8      23306.836588      759.238254    34383.184896    1934.546688\n",
    "#    9      21459.769531      745.624998    33509.142578    1887.377024\n",
    "#    11     19215.382812      641.388842    32197.832682    1734.456935\n",
    "#    12     18627.388021      716.257152    31770.852865    1802.155484\n",
    "#    13     17960.694661      557.043073    31482.782552    1779.123767\n",
    "#    14     17559.736328      631.412555    31389.990886    1892.319967\n",
    "#    15     17205.712565      590.171852    31302.882162    1955.165902\n",
    "#    16     16876.571940      703.631755    31234.058594    1880.705796\n",
    "#    17     16597.662110      703.677609    31318.348959    1828.860617\n",
    "#    18     16330.460937      607.274494    31323.634115    1775.908526\n",
    "#    19     16005.972982      520.470911    31204.134766    1739.075860\n",
    "#    20     15814.301432      518.604477    31089.862630    1756.021674\n",
    "#    21     15493.405599      505.616447    31047.996094    1624.673955\n",
    "#    23     15086.381836      503.912899    31024.983724    1548.985354\n",
    "#    25     14709.589518      449.668010    30989.476563    1686.667469\n",
    "#    26     14457.286458      376.787206    30952.113932    1613.172643\n",
    "#    27     14185.567383      383.102234    31066.902344    1648.534310\n",
    "#    28     13934.066732      473.465449    31095.641276    1709.225654\n",
    "#    29     13749.645182      473.670437    31103.887370    1778.880069\n",
    "#    30     13549.836914      454.898399    30976.085938    1744.515079\n",
    "#    31     13413.485351      399.603618    30938.469401    1746.052445\n",
    "#    32     13275.916016      415.408786    30931.001302    1772.469115\n",
    "#    33     13085.878255      493.792509    30929.057291    1765.540568\n",
    "#    34     12947.181315      517.789746    30890.630208    1786.511392\n",
    "#    35     12846.027344      547.732372    30884.493490    1769.729215\n",
    "#    36     12702.379232      505.523221    30833.542969    1691.002065\n",
    "#    37     12532.244141      508.298241    30856.687500    1771.445978\n",
    "#    38     12384.055013      536.225042    30818.016927    1782.784630\n",
    "#    39     12198.444010      545.165197    30839.393229    1847.327435\n",
    "#    40     12054.583659      508.841772    30776.966146    1912.780507\n",
    "#    41     11897.036133      477.177991    30794.702474    1919.674832\n",
    "#    42     11756.221354      502.992395    30780.955078    1906.820029\n",
    "#    43     11618.846029      519.837502    30783.755860    1951.260705\n",
    "#    44     11484.080404      578.428621    30776.731120    1953.446309\n",
    "#    45     11356.553060      565.368380    30758.544271    1947.455425\n",
    "#    46     11193.558268      552.298848    30729.972656    1985.699788\n",
    "#    47     11071.315429      604.089960    30732.663411    1966.997809\n",
    "#    48     10950.777995      574.863209    30712.241536    1957.751573\n",
    "#    49     10824.865560      576.665405    30720.854167    1950.511057\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Overview of XGBoost's hyperparameters**\n",
    "___\n",
    "- Common tree tunable parameters\n",
    "    - **learning rate** - eta / how quickly model fits residual error using additional base learners. Low learning rate requires more base learners\n",
    "    - **gamma** - minimum loss reduction to create a new split\n",
    "    - **lambda** - L2 regularization on leaf weights\n",
    "    - **alpha** - L1 regularization on leaf weights\n",
    "    - **max_depth** - [integer] max depth per tree per boosting round\n",
    "    - **subsample** - [0-1] % of samples used per tree/boosting round\n",
    "        - too low - underfitting\n",
    "        - too high - overfitting\n",
    "    - **colsample_bytree** - [0-1] % of features used per tree\n",
    "        - smaller values provide additional regularization\n",
    "        - larger values may overfit\n",
    "- Linear tunable parameters\n",
    "    - **lambda**: L2 regularization on weights\n",
    "    - **alpha**: L1 regularization on weights\n",
    "    - **lambda_bias**: L2 regularization term on bias\n",
    "- you can also tune the number of estimators used for both base model types\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Tuning eta\n",
    "\n",
    "#It's time to practice tuning other XGBoost hyperparameters in earnest\n",
    "#and observing their effect on model performance! You'll begin by tuning\n",
    "#the \"eta\", also known as the learning rate.\n",
    "\n",
    "#The learning rate in XGBoost is a parameter that can range between 0\n",
    "#and 1, with higher values of \"eta\" penalizing feature weights more\n",
    "#strongly, causing much stronger regularization.\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "#params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "#eta_vals = [0.001, 0.01, 0.1]\n",
    "#best_rmse = []\n",
    "\n",
    "# Systematically vary the eta\n",
    "#for curr_val in eta_vals:\n",
    "\n",
    "#    params[\"eta\"] = curr_val\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "#    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
    "#                        num_boost_round=10, early_stopping_rounds=5,\n",
    "#                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "    # Append the final round rmse to best_rmse\n",
    "#    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "#print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))\n",
    "\n",
    "#################################################\n",
    "#      eta      best_rmse\n",
    "#    0  0.001  195736.406250\n",
    "#    1  0.010  179932.187500\n",
    "#    2  0.100   79759.411458\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Tuning max_depth\n",
    "\n",
    "#In this exercise, your job is to tune max_depth, which is the parameter\n",
    "#that dictates the maximum depth that each tree in a boosting round can\n",
    "#grow to. Smaller values will lead to shallower trees, and larger values\n",
    "#to deeper trees.\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "#params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "#max_depths = [2, 5, 10, 20]\n",
    "#best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "#for curr_val in max_depths:\n",
    "\n",
    "#    params[\"max_depth\"] = curr_val\n",
    "\n",
    "    # Perform cross-validation\n",
    "#    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "#                 num_boost_round=10, early_stopping_rounds=5,\n",
    "#                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "    # Append the final round rmse to best_rmse\n",
    "#    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "#print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))\n",
    "\n",
    "#################################################\n",
    "#     max_depth     best_rmse\n",
    "#            2     37957.468750\n",
    "#            5     35596.599610\n",
    "#           10     36065.548829\n",
    "#           20     36739.578125\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Tuning colsample_bytree\n",
    "\n",
    "#Now, it's time to tune \"colsample_bytree\". You've already seen this\n",
    "#if you've ever worked with scikit-learn's RandomForestClassifier or\n",
    "#RandomForestRegressor, where it just was called max_features. In both\n",
    "#xgboost and sklearn, this parameter (although named differently) simply\n",
    "#specifies the fraction of features to choose from at every split in a\n",
    "#given tree. In xgboost, colsample_bytree must be specified as a float\n",
    "#between 0 and 1.\n",
    "\n",
    "# Create your housing DMatrix\n",
    "#housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "#params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values\n",
    "#colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "#best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value\n",
    "#for curr_val in colsample_bytree_vals:\n",
    "\n",
    "#    params[\"colsample_bytree\"] = curr_val\n",
    "\n",
    "    # Perform cross-validation\n",
    "#    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "#                 num_boost_round=10, early_stopping_rounds=5,\n",
    "#                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "    # Append the final round rmse to best_rmse\n",
    "#    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "#print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))\n",
    "\n",
    "#################################################\n",
    "#colsample_bytree     best_rmse\n",
    "#    0               0.1  48193.451172\n",
    "#    1               0.5  36013.544922\n",
    "#    2               0.8  35932.962891\n",
    "#    3               1.0  35836.044922\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Review of grid search and random search**\n",
    "___\n",
    "- **Grid search**\n",
    "    - search exhaustively over a given set of hyperparameters, once per set of hyperparameters\n",
    "- **Random search**\n",
    "    - create a range of hyperparameter values per hyperparameter that you would like to search over\n",
    "    - set the number of iterations for the search\n",
    "    - during each iteration, draw a random value for each hyperparameter searched over and train a model with those hyperparameters\n",
    "    - after max iterations have completed, select best evaluated score with corresponding hyperparameter values\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Grid search with XGBoost\n",
    "\n",
    "#Now that you've learned how to tune parameters individually with XGBoost,\n",
    "#let's take your parameter tuning to the next level by using scikit-learn's\n",
    "#GridSearch and RandomizedSearch capabilities with internal cross-validation\n",
    "#using the GridSearchCV and RandomizedSearchCV functions. You will use these\n",
    "#to find the best model exhaustively from a collection of possible parameter\n",
    "#values across multiple parameters simultaneously. Let's get to work, starting\n",
    "#with GridSearchCV!\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "#gbm_param_grid = {\n",
    "#    'colsample_bytree': [0.3, 0.7],\n",
    "#    'n_estimators': [50],\n",
    "#    'max_depth': [2, 5]\n",
    "#}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "#gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "#grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "#                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "#grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "#print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "#print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "\n",
    "#################################################\n",
    "#Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
    "#    Lowest RMSE found:  29916.562522854438\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Random search with XGBoost\n",
    "\n",
    "#Often, GridSearchCV can be really time consuming, so in practice, you\n",
    "#may want to use RandomizedSearchCV instead, as you will do in this\n",
    "#exercise. The good news is you only have to make a few modifications\n",
    "#to your GridSearchCV code to do RandomizedSearchCV. The key difference\n",
    "#is you have to specify a param_distributions parameter instead of a\n",
    "#param_grid parameter.\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "#gbm_param_grid = {\n",
    "#    'n_estimators': [25],\n",
    "#    'max_depth': range(2, 12)\n",
    "#}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "#gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "#randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n",
    "#                                    n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "#randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "#print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "#print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))\n",
    "\n",
    "#################################################\n",
    "#Best parameters found:  {'n_estimators': 25, 'max_depth': 6}\n",
    "#    Lowest RMSE found:  36909.98213965752\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Limits of grid search and random search**\n",
    "___\n",
    "- Grid Search takes a lot of time, especially as number of hyperparameter values grows\n",
    "- Random Search problems are similar, especially related to total number of hyperparameters\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Review of pipelines using sklearn**\n",
    "___\n",
    "- takes a list of named 2-tuples (name, pipeline_step) as input\n",
    "- tuples can contain any arbitrary scikit-learn compatible estimator or transformer object\n",
    "- pipeline implements fit/predict methods\n",
    "- can be used as input estimator into grid/randomized search and cross_val_score methods\n",
    "___\n",
    "**Preprocessing I**\n",
    "    - *LabelEncoder* - converts categorical column of strings into integers\n",
    "    - *OneHotEncoder* - takes the column of integers and encodes them as dummy variables\n",
    "**Prepropressing II**\n",
    "    - *DictVectorizer* - converts lists of feature mappings into vectors\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoding categorical columns I: LabelEncoder\n",
    "\n",
    "#Now that you've seen what will need to be done to get the housing data\n",
    "#ready for XGBoost, let's go through the process step-by-step.\n",
    "\n",
    "#First, you will need to fill in missing values - as you saw previously,\n",
    "#the column LotFrontage has many missing values. Then, you will need to\n",
    "#encode any categorical columns in the dataset using one-hot encoding so\n",
    "#that they are encoded numerically.\n",
    "\n",
    "#The data has five categorical columns: MSZoning, PavedDrive, Neighborhood,\n",
    "#BldgType, and HouseStyle. Scikit-learn has a LabelEncoder function that\n",
    "#converts the values in each categorical column into integers. You'll practice\n",
    "#using this here.\n",
    "\n",
    "# Import LabelEncoder\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "#df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "#categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "#categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "#print(df[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "#le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "#df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "#print(df[categorical_columns].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#      MSZoning PavedDrive Neighborhood BldgType HouseStyle\n",
    "#    0       RL          Y      CollgCr     1Fam     2Story\n",
    "#    1       RL          Y      Veenker     1Fam     1Story\n",
    "#    2       RL          Y      CollgCr     1Fam     2Story\n",
    "#    3       RL          Y      Crawfor     1Fam     2Story\n",
    "#    4       RL          Y      NoRidge     1Fam     2Story\n",
    "#       MSZoning  PavedDrive  Neighborhood  BldgType  HouseStyle\n",
    "#    0         3           2             5         0           5\n",
    "#    1         3           2            24         0           2\n",
    "#    2         3           2             5         0           5\n",
    "#    3         3           2             6         0           5\n",
    "#    4         3           2            15         0           5\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoding categorical columns II: OneHotEncoder\n",
    "\n",
    "#Okay - so you have your categorical columns encoded numerically. Can you\n",
    "#now move onto using pipelines and XGBoost? Not yet! In the categorical\n",
    "#columns of this dataset, there is no natural ordering between the entries.\n",
    "#As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded\n",
    "#as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6.\n",
    "#Is Veenker \"greater\" than Crawfor and CollgCr? No - and allowing the\n",
    "#model to assume this natural ordering may result in poor performance.\n",
    "\n",
    "#As a result, there is another step needed: You have to apply a one-hot\n",
    "#encoding to create binary, or \"dummy\" variables. You can do this using\n",
    "#scikit-learn's OneHotEncoder.\n",
    "\n",
    "# Import OneHotEncoder\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "#ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "#df_encoded = ohe.fit_transform(df)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "#print(df_encoded[:5, :])\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "#print(df.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "#print(df_encoded.shape)\n",
    "\n",
    "#################################################\n",
    "#    (1460, 21)\n",
    "#    (1460, 62)\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoding categorical columns III: DictVectorizer\n",
    "\n",
    "#Alright, one final trick before you dive into pipelines. The two step\n",
    "#process you just went through - LabelEncoder followed by OneHotEncoder\n",
    "#- can be simplified by using a DictVectorizer.\n",
    "\n",
    "#Using a DictVectorizer on a DataFrame that has been converted to a\n",
    "#dictionary allows you to get label encoding as well as one-hot\n",
    "#encoding in one go.\n",
    "\n",
    "#Your task is to work through this strategy in this exercise!\n",
    "\n",
    "# Import DictVectorizer\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "#df_dict = df.to_dict(\"records\")\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "#dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "#df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "#print(df_encoded[:5,:])\n",
    "\n",
    "# Print the vocabulary\n",
    "#print(dv.vocabulary_)\n",
    "\n",
    "#################################################\n",
    "#{'MSSubClass': 23, 'LotFrontage': 22, 'LotArea': 21, 'OverallQual': 55,\n",
    "#'OverallCond': 54, 'YearBuilt': 61, 'Remodeled': 59, 'GrLivArea': 11,\n",
    "#'BsmtFullBath': 6, 'BsmtHalfBath': 7, 'FullBath': 9, 'HalfBath': 12,\n",
    "#'BedroomAbvGr': 0, 'Fireplaces': 8, 'GarageArea': 10, 'MSZoning=RL': 27,\n",
    "#'PavedDrive=Y': 58, 'Neighborhood=CollgCr': 34, 'BldgType=1Fam': 1,\n",
    "#'HouseStyle=2Story': 18, 'SalePrice': 60, 'Neighborhood=Veenker': 53,\n",
    "#'HouseStyle=1Story': 15, 'Neighborhood=Crawfor': 35, 'Neighborhood=NoRidge': 44,\n",
    "#'Neighborhood=Mitchel': 40, 'HouseStyle=1.5Fin': 13, 'Neighborhood=Somerst': 50,\n",
    "#'Neighborhood=NWAmes': 43, 'MSZoning=RM': 28, 'Neighborhood=OldTown': 46,\n",
    "#'Neighborhood=BrkSide': 32, 'BldgType=2fmCon': 2, 'HouseStyle=1.5Unf': 14,\n",
    "#'Neighborhood=Sawyer': 48, 'Neighborhood=NridgHt': 45, 'Neighborhood=NAmes': 41,\n",
    "#'BldgType=Duplex': 3, 'Neighborhood=SawyerW': 49, 'PavedDrive=N': 56,\n",
    "#'Neighborhood=IDOTRR': 38, 'Neighborhood=MeadowV': 39, 'BldgType=TwnhsE': 5,\n",
    "#'MSZoning=C (all)': 24, 'Neighborhood=Edwards': 36, 'PavedDrive=P': 57,\n",
    "#'Neighborhood=Timber': 52, 'HouseStyle=SFoyer': 19, 'MSZoning=FV': 25,\n",
    "#'Neighborhood=Gilbert': 37, 'HouseStyle=SLvl': 20, 'BldgType=Twnhs': 4,\n",
    "#'Neighborhood=StoneBr': 51, 'HouseStyle=2.5Unf': 17, 'Neighborhood=ClearCr': 33,\n",
    "#'Neighborhood=NPkVill': 42, 'HouseStyle=2.5Fin': 16, 'Neighborhood=Blmngtn': 29,\n",
    "#'Neighborhood=BrDale': 31, 'Neighborhood=SWISU': 47, 'MSZoning=RH': 26,\n",
    "#'Neighborhood=Blueste': 30}\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Preprocessing within a pipeline\n",
    "\n",
    "#Now that you've seen what steps need to be taken individually to\n",
    "#properly process the Ames housing data, let's use the much cleaner\n",
    "#and more succinct DictVectorizer approach and put it alongside an\n",
    "#XGBoostRegressor inside of a scikit-learn pipeline.\n",
    "\n",
    "# Import necessary modules\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "#X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "#steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "#         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "#xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "#xgb_pipeline.fit(X.to_dict(\"records\"), y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Incorporating XGBoost into pipelines**\n",
    "___\n",
    "- pipeline behavior is the same for other scikit-learn algorithms\n",
    "- advanced data wrangling using additional components is included in examples below:\n",
    "    - sklearn_pandas (This library is not really maintained anymore, so let's do preprocessing differently)\n",
    "        - DataFrameMapper - interoperability between pandas and scikit-learn\n",
    "        - CategoricalImputer - allow for imputation of categorical variables before conversion to integers\n",
    "    - sklearn.preprocessing\n",
    "        - Imputer - Native imputation of numerical columns in scikit-learn\n",
    "    - sklearn.pipeline\n",
    "        - FeatureUnion - combine multiple pipelines of features into a single pipeline of features\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Cross-validating your XGBoost model\n",
    "\n",
    "#In this exercise, you'll go one step further by using the pipeline\n",
    "#you've created to preprocess and cross-validate your model.\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "#X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "#steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "#         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "#xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "#cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "#print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))\n",
    "\n",
    "#################################################\n",
    "#10-fold RMSE:  29867.603720688923\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Kidney disease case study I: Categorical Imputer\n",
    "\n",
    "#You'll now continue your exploration of using pipelines with a\n",
    "#dataset that requires significantly more wrangling. The chronic\n",
    "#kidney disease dataset contains both categorical and numeric\n",
    "#eatures, but contains lots of missing values. The goal here is to\n",
    "#predict who has chronic kidney disease given various blood\n",
    "#indicators as features.\n",
    "\n",
    "#As Sergey mentioned in the video, you'll be introduced to a new\n",
    "#library, sklearn_pandas, that allows you to chain many more\n",
    "#processing steps inside of a pipeline than are currently supported\n",
    "#in scikit-learn. Specifically, you'll be able to impute missing\n",
    "#categorical values directly using the Categorical_Imputer() class\n",
    "#in sklearn_pandas, and the DataFrameMapper() class to apply any\n",
    "#arbitrary sklearn-compatible transformer on DataFrame columns,\n",
    "#where the resulting output can be either a NumPy array or DataFrame.\n",
    "\n",
    "#We've also created a transformer called a Dictifier that encapsulates\n",
    "#converting a DataFrame using .to_dict(\"records\") without you having\n",
    "#to do it explicitly (and so that it works in a pipeline). Finally,\n",
    "#we've also provided the list of feature names in kidney_feature_names,\n",
    "#the target name in kidney_target_name, the features in X, and the\n",
    "#target in y.\n",
    "\n",
    "#In this exercise, your task is to apply the CategoricalImputer to\n",
    "#impute all of the categorical columns in the dataset. You can refer\n",
    "#to how the numeric imputation mapper was created as a template.\n",
    "#Notice the keyword arguments input_df=True and df_out=True? This is\n",
    "#so that you can work with DataFrames instead of arrays. By default,\n",
    "#the transformers are passed a numpy array of the selected columns as\n",
    "#input, and as a result, the output of the DataFrame mapper is also an\n",
    "#array. Scikit-learn transformers have historically been designed to\n",
    "#work with numpy arrays, not pandas DataFrames, even though their basic\n",
    "#indexing interfaces are similar.\n",
    "\n",
    "# Import necessary modules\n",
    "#from sklearn_pandas import DataFrameMapper\n",
    "#from sklearn_pandas import CategoricalImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "#nulls_per_column = X.isnull().sum()\n",
    "#print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "#categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "#categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "#non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "#numeric_imputation_mapper = DataFrameMapper(\n",
    "#                                            [([numeric_feature],Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "#                                            input_df=True,\n",
    "#                                            df_out=True\n",
    "#                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "#categorical_imputation_mapper = DataFrameMapper(\n",
    "#                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
    "#                                                input_df=True,\n",
    "#                                                df_out=True\n",
    "#                                               )\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    age        9\n",
    "#    bp        12\n",
    "#    sg        47\n",
    "#    al        46\n",
    "#    su        49\n",
    "#    bgr       44\n",
    "#    bu        19\n",
    "#    sc        17\n",
    "#    sod       87\n",
    "#    pot       88\n",
    "#    hemo      52\n",
    "#    pcv       71\n",
    "#    wc       106\n",
    "#    rc       131\n",
    "#    rbc      152\n",
    "#    pc        65\n",
    "#    pcc        4\n",
    "#    ba         4\n",
    "#    htn        2\n",
    "#    dm         2\n",
    "#    cad        2\n",
    "#    appet      1\n",
    "#    pe         1\n",
    "#    ane        1\n",
    "#    dtype: int64\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Kidney disease case study II: Feature Union\n",
    "\n",
    "#Having separately imputed numeric as well as categorical columns,\n",
    "#your task is now to use scikit-learn's FeatureUnion to concatenate\n",
    "#their results, which are contained in two separate transformer objects\n",
    "#- numeric_imputation_mapper, and categorical_imputation_mapper,\n",
    "#respectively.\n",
    "\n",
    "#You may have already encountered FeatureUnion in Machine Learning\n",
    "#with the Experts: School Budgets. Just like with pipelines, you\n",
    "#have to pass it a list of (string, transformer) tuples, where the\n",
    "#first half of each tuple is the name of the transformer.\n",
    "\n",
    "# Import FeatureUnion\n",
    "#from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "#numeric_categorical_union = FeatureUnion([\n",
    "#                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "#                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "#                                         ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Kidney disease case study III: Full pipeline\n",
    "\n",
    "#It's time to piece together all of the transforms along with an\n",
    "#XGBClassifier to build the full pipeline!\n",
    "\n",
    "#Besides the numeric_categorical_union that you created in the\n",
    "#previous exercise, there are two other transforms needed: the\n",
    "#Dictifier() transform which we created for you, and the\n",
    "#DictVectorizer().\n",
    "\n",
    "#After creating the pipeline, your task is to cross-validate it to\n",
    "#see how well it performs.\n",
    "\n",
    "# Create full pipeline\n",
    "#pipeline = Pipeline([\n",
    "#                     (\"featureunion\", numeric_categorical_union),\n",
    "#                     (\"dictifier\", Dictifier()),\n",
    "#                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "#                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "#                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "#cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "# Print avg. AUC\n",
    "#print(\"3-fold AUC: \", np.mean(cross_val_scores))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    3-fold AUC:  0.998637406769937\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tuning XGBoost hyperparameters**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bringing it all together\n",
    "\n",
    "#Alright, it's time to bring together everything you've learned so\n",
    "#far! In this final exercise of the course, you will combine your\n",
    "#work from the previous exercises into one end-to-end XGBoost\n",
    "#pipeline to really cement your understanding of preprocessing and\n",
    "#pipelines in XGBoost.\n",
    "\n",
    "#Your work from the previous 3 exercises, where you preprocessed the\n",
    "#data and set up your pipeline, has been pre-loaded. Your job is to\n",
    "#perform a randomized search and identify the best hyperparameters.\n",
    "\n",
    "# Create the parameter grid\n",
    "#gbm_param_grid = {\n",
    "#    'clf__learning_rate': np.arange(.05, 1, .05),\n",
    "#    'clf__max_depth': np.arange(3,10, 1),\n",
    "#    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "#}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "#randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
    "#                                        param_distributions=gbm_param_grid,\n",
    "#                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "#randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "#print(randomized_roc_auc.best_score_)\n",
    "#print(randomized_roc_auc.best_estimator_)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
    "#    0.9965333333333334\n",
    "#    Pipeline(memory=None,\n",
    "#             steps=[('featureunion',\n",
    "#                     FeatureUnion(n_jobs=None,\n",
    "#                                  transformer_list=[('num_mapper',\n",
    "#                                                     DataFrameMapper(default=False,\n",
    "#                                                                     df_out=True,\n",
    "#                                                                     features=[(['age'],\n",
    "#                                                                                Imputer(axis=0,\n",
    "#                                                                                        copy=True,\n",
    "#                                                                                        missing_values='NaN',\n",
    "#                                                                                        strategy='median',\n",
    "#                                                                                        verbose=0)),\n",
    "#                                                                               (['bp'],\n",
    "#                                                                                Imputer(axis=0,\n",
    "#                                                                                        copy=True,\n",
    "#                                                                                        missing_values='NaN',\n",
    "#                                                                                        strategy='median',\n",
    "#                                                                                        verbose=0)),\n",
    "#                                                                               (['sg'],\n",
    "#                                                                                Imputer(axis=0,\n",
    "#                                                                                        copy=...\n",
    "#                     XGBClassifier(base_score=0.5, booster='gbtree',\n",
    "#                                   colsample_bylevel=1, colsample_bynode=1,\n",
    "#                                   colsample_bytree=1, gamma=0,\n",
    "#                                   learning_rate=0.9500000000000001,\n",
    "#                                   max_delta_step=0, max_depth=4,\n",
    "#                                   min_child_weight=1, missing=None,\n",
    "#                                   n_estimators=100, n_jobs=1, nthread=None,\n",
    "#                                   objective='binary:logistic', random_state=0,\n",
    "#                                   reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "#                                   seed=None, silent=None, subsample=1,\n",
    "#                                   verbosity=1))],\n",
    "#             verbose=False)\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Final Thoughts**\n",
    "___\n",
    "- Using XGBoost for classification and regression tasks\n",
    "- Tuning XGBoost's most important hyperparameters\n",
    "- Incorporating XGBoost into sklearn pipelines\n",
    "- Not covered:\n",
    "    - using XGBoost for ranking/recommendation problems (Netflix/Amazon)\n",
    "    - using more sophisticated hyperparameter tuning strategies for tuning XGBoost models (Bayesian Optimization)\n",
    "    - using XGBoost as part of an ensemble of other models for regression/classification"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}